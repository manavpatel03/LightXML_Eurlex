{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JPFu3XwCx19n",
        "outputId": "91080ca1-7660-48a4-acf7-6eacfe1b8a52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.7/536.7 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.3/247.3 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.8/75.8 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m709.8/709.8 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.0/146.0 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.0/115.0 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.3/247.3 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.7/133.7 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/41.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.6/46.6 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for apex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for velruse (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for cryptacular (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for anykeystore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pbkdf2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Cloning into 'apex'...\n",
            "remote: Enumerating objects: 11632, done.\u001b[K\n",
            "remote: Counting objects: 100% (3725/3725), done.\u001b[K\n",
            "remote: Compressing objects: 100% (573/573), done.\u001b[K\n",
            "remote: Total 11632 (delta 3370), reused 3277 (delta 3149), pack-reused 7907\u001b[K\n",
            "Receiving objects: 100% (11632/11632), 15.46 MiB | 10.56 MiB/s, done.\n",
            "Resolving deltas: 100% (8172/8172), done.\n",
            "/content/apex\n",
            "\n",
            "\n",
            "torch.__version__  = 2.1.0+cu121\n",
            "\n",
            "\n",
            "running install\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` directly.\n",
            "        Instead, use pypa/build, pypa/installer, pypa/build or\n",
            "        other standards-based tools.\n",
            "\n",
            "        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  self.initialize_options()\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/cmd.py:66: EasyInstallDeprecationWarning: easy_install command is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` and ``easy_install``.\n",
            "        Instead, use pypa/build, pypa/installer, pypa/build or\n",
            "        other standards-based tools.\n",
            "\n",
            "        See https://github.com/pypa/setuptools/issues/917 for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  self.initialize_options()\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "creating apex.egg-info\n",
            "writing apex.egg-info/PKG-INFO\n",
            "writing dependency_links to apex.egg-info/dependency_links.txt\n",
            "writing requirements to apex.egg-info/requires.txt\n",
            "writing top-level names to apex.egg-info/top_level.txt\n",
            "writing manifest file 'apex.egg-info/SOURCES.txt'\n",
            "reading manifest file 'apex.egg-info/SOURCES.txt'\n",
            "adding license file 'LICENSE'\n",
            "writing manifest file 'apex.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "running build_py\n",
            "creating build\n",
            "creating build/lib\n",
            "creating build/lib/apex\n",
            "copying apex/_autocast_utils.py -> build/lib/apex\n",
            "copying apex/__init__.py -> build/lib/apex\n",
            "creating build/lib/apex/amp\n",
            "copying apex/amp/amp.py -> build/lib/apex/amp\n",
            "copying apex/amp/utils.py -> build/lib/apex/amp\n",
            "copying apex/amp/compat.py -> build/lib/apex/amp\n",
            "copying apex/amp/__version__.py -> build/lib/apex/amp\n",
            "copying apex/amp/handle.py -> build/lib/apex/amp\n",
            "copying apex/amp/wrap.py -> build/lib/apex/amp\n",
            "copying apex/amp/_initialize.py -> build/lib/apex/amp\n",
            "copying apex/amp/__init__.py -> build/lib/apex/amp\n",
            "copying apex/amp/scaler.py -> build/lib/apex/amp\n",
            "copying apex/amp/frontend.py -> build/lib/apex/amp\n",
            "copying apex/amp/_amp_state.py -> build/lib/apex/amp\n",
            "copying apex/amp/_process_optimizer.py -> build/lib/apex/amp\n",
            "copying apex/amp/rnn_compat.py -> build/lib/apex/amp\n",
            "copying apex/amp/opt.py -> build/lib/apex/amp\n",
            "creating build/lib/apex/contrib\n",
            "copying apex/contrib/__init__.py -> build/lib/apex/contrib\n",
            "creating build/lib/apex/RNN\n",
            "copying apex/RNN/cells.py -> build/lib/apex/RNN\n",
            "copying apex/RNN/models.py -> build/lib/apex/RNN\n",
            "copying apex/RNN/__init__.py -> build/lib/apex/RNN\n",
            "copying apex/RNN/RNNBackend.py -> build/lib/apex/RNN\n",
            "creating build/lib/apex/parallel\n",
            "copying apex/parallel/distributed.py -> build/lib/apex/parallel\n",
            "copying apex/parallel/multiproc.py -> build/lib/apex/parallel\n",
            "copying apex/parallel/optimized_sync_batchnorm_kernel.py -> build/lib/apex/parallel\n",
            "copying apex/parallel/__init__.py -> build/lib/apex/parallel\n",
            "copying apex/parallel/sync_batchnorm.py -> build/lib/apex/parallel\n",
            "copying apex/parallel/sync_batchnorm_kernel.py -> build/lib/apex/parallel\n",
            "copying apex/parallel/LARC.py -> build/lib/apex/parallel\n",
            "copying apex/parallel/optimized_sync_batchnorm.py -> build/lib/apex/parallel\n",
            "creating build/lib/apex/optimizers\n",
            "copying apex/optimizers/fused_sgd.py -> build/lib/apex/optimizers\n",
            "copying apex/optimizers/fused_mixed_precision_lamb.py -> build/lib/apex/optimizers\n",
            "copying apex/optimizers/__init__.py -> build/lib/apex/optimizers\n",
            "copying apex/optimizers/fused_novograd.py -> build/lib/apex/optimizers\n",
            "copying apex/optimizers/fused_adagrad.py -> build/lib/apex/optimizers\n",
            "copying apex/optimizers/fused_lamb.py -> build/lib/apex/optimizers\n",
            "copying apex/optimizers/fused_adam.py -> build/lib/apex/optimizers\n",
            "creating build/lib/apex/transformer\n",
            "copying apex/transformer/log_util.py -> build/lib/apex/transformer\n",
            "copying apex/transformer/parallel_state.py -> build/lib/apex/transformer\n",
            "copying apex/transformer/utils.py -> build/lib/apex/transformer\n",
            "copying apex/transformer/enums.py -> build/lib/apex/transformer\n",
            "copying apex/transformer/__init__.py -> build/lib/apex/transformer\n",
            "copying apex/transformer/microbatches.py -> build/lib/apex/transformer\n",
            "copying apex/transformer/_ucc_util.py -> build/lib/apex/transformer\n",
            "creating build/lib/apex/multi_tensor_apply\n",
            "copying apex/multi_tensor_apply/__init__.py -> build/lib/apex/multi_tensor_apply\n",
            "copying apex/multi_tensor_apply/multi_tensor_apply.py -> build/lib/apex/multi_tensor_apply\n",
            "creating build/lib/apex/fp16_utils\n",
            "copying apex/fp16_utils/fp16util.py -> build/lib/apex/fp16_utils\n",
            "copying apex/fp16_utils/loss_scaler.py -> build/lib/apex/fp16_utils\n",
            "copying apex/fp16_utils/__init__.py -> build/lib/apex/fp16_utils\n",
            "copying apex/fp16_utils/fp16_optimizer.py -> build/lib/apex/fp16_utils\n",
            "creating build/lib/apex/mlp\n",
            "copying apex/mlp/mlp.py -> build/lib/apex/mlp\n",
            "copying apex/mlp/__init__.py -> build/lib/apex/mlp\n",
            "creating build/lib/apex/fused_dense\n",
            "copying apex/fused_dense/__init__.py -> build/lib/apex/fused_dense\n",
            "copying apex/fused_dense/fused_dense.py -> build/lib/apex/fused_dense\n",
            "creating build/lib/apex/normalization\n",
            "copying apex/normalization/__init__.py -> build/lib/apex/normalization\n",
            "copying apex/normalization/fused_layer_norm.py -> build/lib/apex/normalization\n",
            "creating build/lib/apex/amp/lists\n",
            "copying apex/amp/lists/functional_overrides.py -> build/lib/apex/amp/lists\n",
            "copying apex/amp/lists/__init__.py -> build/lib/apex/amp/lists\n",
            "copying apex/amp/lists/torch_overrides.py -> build/lib/apex/amp/lists\n",
            "copying apex/amp/lists/tensor_overrides.py -> build/lib/apex/amp/lists\n",
            "creating build/lib/apex/contrib/test\n",
            "copying apex/contrib/test/__init__.py -> build/lib/apex/contrib/test\n",
            "creating build/lib/apex/contrib/group_norm\n",
            "copying apex/contrib/group_norm/__init__.py -> build/lib/apex/contrib/group_norm\n",
            "copying apex/contrib/group_norm/group_norm.py -> build/lib/apex/contrib/group_norm\n",
            "creating build/lib/apex/contrib/index_mul_2d\n",
            "copying apex/contrib/index_mul_2d/__init__.py -> build/lib/apex/contrib/index_mul_2d\n",
            "copying apex/contrib/index_mul_2d/index_mul_2d.py -> build/lib/apex/contrib/index_mul_2d\n",
            "creating build/lib/apex/contrib/cudnn_gbn\n",
            "copying apex/contrib/cudnn_gbn/batch_norm.py -> build/lib/apex/contrib/cudnn_gbn\n",
            "copying apex/contrib/cudnn_gbn/__init__.py -> build/lib/apex/contrib/cudnn_gbn\n",
            "creating build/lib/apex/contrib/openfold_triton\n",
            "copying apex/contrib/openfold_triton/mha.py -> build/lib/apex/contrib/openfold_triton\n",
            "copying apex/contrib/openfold_triton/_layer_norm_config_hopper.py -> build/lib/apex/contrib/openfold_triton\n",
            "copying apex/contrib/openfold_triton/__init__.py -> build/lib/apex/contrib/openfold_triton\n",
            "copying apex/contrib/openfold_triton/_layer_norm_config_ampere.py -> build/lib/apex/contrib/openfold_triton\n",
            "copying apex/contrib/openfold_triton/_layer_norm_backward_kernels.py -> build/lib/apex/contrib/openfold_triton\n",
            "copying apex/contrib/openfold_triton/fused_adam_swa.py -> build/lib/apex/contrib/openfold_triton\n",
            "copying apex/contrib/openfold_triton/_layer_norm_forward_kernels.py -> build/lib/apex/contrib/openfold_triton\n",
            "copying apex/contrib/openfold_triton/layer_norm.py -> build/lib/apex/contrib/openfold_triton\n",
            "copying apex/contrib/openfold_triton/_mha_kernel.py -> build/lib/apex/contrib/openfold_triton\n",
            "creating build/lib/apex/contrib/sparsity\n",
            "copying apex/contrib/sparsity/asp.py -> build/lib/apex/contrib/sparsity\n",
            "copying apex/contrib/sparsity/permutation_lib.py -> build/lib/apex/contrib/sparsity\n",
            "copying apex/contrib/sparsity/__init__.py -> build/lib/apex/contrib/sparsity\n",
            "copying apex/contrib/sparsity/sparse_masklib.py -> build/lib/apex/contrib/sparsity\n",
            "creating build/lib/apex/contrib/xentropy\n",
            "copying apex/contrib/xentropy/__init__.py -> build/lib/apex/contrib/xentropy\n",
            "copying apex/contrib/xentropy/softmax_xentropy.py -> build/lib/apex/contrib/xentropy\n",
            "creating build/lib/apex/contrib/transducer\n",
            "copying apex/contrib/transducer/_transducer_ref.py -> build/lib/apex/contrib/transducer\n",
            "copying apex/contrib/transducer/__init__.py -> build/lib/apex/contrib/transducer\n",
            "copying apex/contrib/transducer/transducer.py -> build/lib/apex/contrib/transducer\n",
            "creating build/lib/apex/contrib/fmha\n",
            "copying apex/contrib/fmha/__init__.py -> build/lib/apex/contrib/fmha\n",
            "copying apex/contrib/fmha/fmha.py -> build/lib/apex/contrib/fmha\n",
            "creating build/lib/apex/contrib/peer_memory\n",
            "copying apex/contrib/peer_memory/peer_halo_exchanger_1d.py -> build/lib/apex/contrib/peer_memory\n",
            "copying apex/contrib/peer_memory/peer_memory.py -> build/lib/apex/contrib/peer_memory\n",
            "copying apex/contrib/peer_memory/__init__.py -> build/lib/apex/contrib/peer_memory\n",
            "creating build/lib/apex/contrib/groupbn\n",
            "copying apex/contrib/groupbn/batch_norm.py -> build/lib/apex/contrib/groupbn\n",
            "copying apex/contrib/groupbn/__init__.py -> build/lib/apex/contrib/groupbn\n",
            "creating build/lib/apex/contrib/conv_bias_relu\n",
            "copying apex/contrib/conv_bias_relu/__init__.py -> build/lib/apex/contrib/conv_bias_relu\n",
            "copying apex/contrib/conv_bias_relu/conv_bias_relu.py -> build/lib/apex/contrib/conv_bias_relu\n",
            "creating build/lib/apex/contrib/multihead_attn\n",
            "copying apex/contrib/multihead_attn/self_multihead_attn_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "copying apex/contrib/multihead_attn/encdec_multihead_attn.py -> build/lib/apex/contrib/multihead_attn\n",
            "copying apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "copying apex/contrib/multihead_attn/self_multihead_attn.py -> build/lib/apex/contrib/multihead_attn\n",
            "copying apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "copying apex/contrib/multihead_attn/__init__.py -> build/lib/apex/contrib/multihead_attn\n",
            "copying apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "copying apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "creating build/lib/apex/contrib/gpu_direct_storage\n",
            "copying apex/contrib/gpu_direct_storage/__init__.py -> build/lib/apex/contrib/gpu_direct_storage\n",
            "creating build/lib/apex/contrib/optimizers\n",
            "copying apex/contrib/optimizers/fused_sgd.py -> build/lib/apex/contrib/optimizers\n",
            "copying apex/contrib/optimizers/distributed_fused_lamb.py -> build/lib/apex/contrib/optimizers\n",
            "copying apex/contrib/optimizers/distributed_fused_adam.py -> build/lib/apex/contrib/optimizers\n",
            "copying apex/contrib/optimizers/__init__.py -> build/lib/apex/contrib/optimizers\n",
            "copying apex/contrib/optimizers/fp16_optimizer.py -> build/lib/apex/contrib/optimizers\n",
            "copying apex/contrib/optimizers/fused_lamb.py -> build/lib/apex/contrib/optimizers\n",
            "copying apex/contrib/optimizers/fused_adam.py -> build/lib/apex/contrib/optimizers\n",
            "creating build/lib/apex/contrib/clip_grad\n",
            "copying apex/contrib/clip_grad/clip_grad.py -> build/lib/apex/contrib/clip_grad\n",
            "copying apex/contrib/clip_grad/__init__.py -> build/lib/apex/contrib/clip_grad\n",
            "creating build/lib/apex/contrib/bottleneck\n",
            "copying apex/contrib/bottleneck/__init__.py -> build/lib/apex/contrib/bottleneck\n",
            "copying apex/contrib/bottleneck/test.py -> build/lib/apex/contrib/bottleneck\n",
            "copying apex/contrib/bottleneck/bottleneck.py -> build/lib/apex/contrib/bottleneck\n",
            "copying apex/contrib/bottleneck/halo_exchangers.py -> build/lib/apex/contrib/bottleneck\n",
            "creating build/lib/apex/contrib/focal_loss\n",
            "copying apex/contrib/focal_loss/focal_loss.py -> build/lib/apex/contrib/focal_loss\n",
            "copying apex/contrib/focal_loss/__init__.py -> build/lib/apex/contrib/focal_loss\n",
            "creating build/lib/apex/contrib/layer_norm\n",
            "copying apex/contrib/layer_norm/__init__.py -> build/lib/apex/contrib/layer_norm\n",
            "copying apex/contrib/layer_norm/layer_norm.py -> build/lib/apex/contrib/layer_norm\n",
            "creating build/lib/apex/contrib/test/group_norm\n",
            "copying apex/contrib/test/group_norm/test_group_norm.py -> build/lib/apex/contrib/test/group_norm\n",
            "copying apex/contrib/test/group_norm/__init__.py -> build/lib/apex/contrib/test/group_norm\n",
            "creating build/lib/apex/contrib/test/index_mul_2d\n",
            "copying apex/contrib/test/index_mul_2d/test_index_mul_2d.py -> build/lib/apex/contrib/test/index_mul_2d\n",
            "copying apex/contrib/test/index_mul_2d/__init__.py -> build/lib/apex/contrib/test/index_mul_2d\n",
            "creating build/lib/apex/contrib/test/cudnn_gbn\n",
            "copying apex/contrib/test/cudnn_gbn/__init__.py -> build/lib/apex/contrib/test/cudnn_gbn\n",
            "copying apex/contrib/test/cudnn_gbn/test_cudnn_gbn_with_two_gpus.py -> build/lib/apex/contrib/test/cudnn_gbn\n",
            "creating build/lib/apex/contrib/test/xentropy\n",
            "copying apex/contrib/test/xentropy/__init__.py -> build/lib/apex/contrib/test/xentropy\n",
            "copying apex/contrib/test/xentropy/test_label_smoothing.py -> build/lib/apex/contrib/test/xentropy\n",
            "creating build/lib/apex/contrib/test/transducer\n",
            "copying apex/contrib/test/transducer/__init__.py -> build/lib/apex/contrib/test/transducer\n",
            "copying apex/contrib/test/transducer/test_transducer_loss.py -> build/lib/apex/contrib/test/transducer\n",
            "copying apex/contrib/test/transducer/test_transducer_joint.py -> build/lib/apex/contrib/test/transducer\n",
            "creating build/lib/apex/contrib/test/fmha\n",
            "copying apex/contrib/test/fmha/test_fmha.py -> build/lib/apex/contrib/test/fmha\n",
            "copying apex/contrib/test/fmha/__init__.py -> build/lib/apex/contrib/test/fmha\n",
            "creating build/lib/apex/contrib/test/peer_memory\n",
            "copying apex/contrib/test/peer_memory/__init__.py -> build/lib/apex/contrib/test/peer_memory\n",
            "copying apex/contrib/test/peer_memory/test_peer_halo_exchange_module.py -> build/lib/apex/contrib/test/peer_memory\n",
            "creating build/lib/apex/contrib/test/conv_bias_relu\n",
            "copying apex/contrib/test/conv_bias_relu/__init__.py -> build/lib/apex/contrib/test/conv_bias_relu\n",
            "copying apex/contrib/test/conv_bias_relu/test_conv_bias_relu.py -> build/lib/apex/contrib/test/conv_bias_relu\n",
            "creating build/lib/apex/contrib/test/multihead_attn\n",
            "copying apex/contrib/test/multihead_attn/test_encdec_multihead_attn_norm_add.py -> build/lib/apex/contrib/test/multihead_attn\n",
            "copying apex/contrib/test/multihead_attn/test_self_multihead_attn.py -> build/lib/apex/contrib/test/multihead_attn\n",
            "copying apex/contrib/test/multihead_attn/test_self_multihead_attn_norm_add.py -> build/lib/apex/contrib/test/multihead_attn\n",
            "copying apex/contrib/test/multihead_attn/__init__.py -> build/lib/apex/contrib/test/multihead_attn\n",
            "copying apex/contrib/test/multihead_attn/test_fast_self_multihead_attn_bias.py -> build/lib/apex/contrib/test/multihead_attn\n",
            "copying apex/contrib/test/multihead_attn/test_encdec_multihead_attn.py -> build/lib/apex/contrib/test/multihead_attn\n",
            "copying apex/contrib/test/multihead_attn/test_mha_fused_softmax.py -> build/lib/apex/contrib/test/multihead_attn\n",
            "creating build/lib/apex/contrib/test/optimizers\n",
            "copying apex/contrib/test/optimizers/test_distributed_fused_lamb.py -> build/lib/apex/contrib/test/optimizers\n",
            "copying apex/contrib/test/optimizers/test_dist_adam.py -> build/lib/apex/contrib/test/optimizers\n",
            "copying apex/contrib/test/optimizers/__init__.py -> build/lib/apex/contrib/test/optimizers\n",
            "creating build/lib/apex/contrib/test/clip_grad\n",
            "copying apex/contrib/test/clip_grad/test_clip_grad.py -> build/lib/apex/contrib/test/clip_grad\n",
            "copying apex/contrib/test/clip_grad/__init__.py -> build/lib/apex/contrib/test/clip_grad\n",
            "creating build/lib/apex/contrib/test/bottleneck\n",
            "copying apex/contrib/test/bottleneck/test_bottleneck_module.py -> build/lib/apex/contrib/test/bottleneck\n",
            "copying apex/contrib/test/bottleneck/__init__.py -> build/lib/apex/contrib/test/bottleneck\n",
            "creating build/lib/apex/contrib/test/focal_loss\n",
            "copying apex/contrib/test/focal_loss/__init__.py -> build/lib/apex/contrib/test/focal_loss\n",
            "copying apex/contrib/test/focal_loss/test_focal_loss.py -> build/lib/apex/contrib/test/focal_loss\n",
            "creating build/lib/apex/contrib/test/layer_norm\n",
            "copying apex/contrib/test/layer_norm/__init__.py -> build/lib/apex/contrib/test/layer_norm\n",
            "copying apex/contrib/test/layer_norm/test_fast_layer_norm.py -> build/lib/apex/contrib/test/layer_norm\n",
            "creating build/lib/apex/contrib/sparsity/permutation_search_kernels\n",
            "copying apex/contrib/sparsity/permutation_search_kernels/channel_swap.py -> build/lib/apex/contrib/sparsity/permutation_search_kernels\n",
            "copying apex/contrib/sparsity/permutation_search_kernels/call_permutation_search_kernels.py -> build/lib/apex/contrib/sparsity/permutation_search_kernels\n",
            "copying apex/contrib/sparsity/permutation_search_kernels/__init__.py -> build/lib/apex/contrib/sparsity/permutation_search_kernels\n",
            "copying apex/contrib/sparsity/permutation_search_kernels/permutation_utilities.py -> build/lib/apex/contrib/sparsity/permutation_search_kernels\n",
            "copying apex/contrib/sparsity/permutation_search_kernels/exhaustive_search.py -> build/lib/apex/contrib/sparsity/permutation_search_kernels\n",
            "creating build/lib/apex/transformer/layers\n",
            "copying apex/transformer/layers/__init__.py -> build/lib/apex/transformer/layers\n",
            "copying apex/transformer/layers/layer_norm.py -> build/lib/apex/transformer/layers\n",
            "creating build/lib/apex/transformer/testing\n",
            "copying apex/transformer/testing/__init__.py -> build/lib/apex/transformer/testing\n",
            "copying apex/transformer/testing/standalone_bert.py -> build/lib/apex/transformer/testing\n",
            "copying apex/transformer/testing/distributed_test_base.py -> build/lib/apex/transformer/testing\n",
            "copying apex/transformer/testing/arguments.py -> build/lib/apex/transformer/testing\n",
            "copying apex/transformer/testing/commons.py -> build/lib/apex/transformer/testing\n",
            "copying apex/transformer/testing/global_vars.py -> build/lib/apex/transformer/testing\n",
            "copying apex/transformer/testing/standalone_transformer_lm.py -> build/lib/apex/transformer/testing\n",
            "copying apex/transformer/testing/standalone_gpt.py -> build/lib/apex/transformer/testing\n",
            "creating build/lib/apex/transformer/amp\n",
            "copying apex/transformer/amp/grad_scaler.py -> build/lib/apex/transformer/amp\n",
            "copying apex/transformer/amp/__init__.py -> build/lib/apex/transformer/amp\n",
            "creating build/lib/apex/transformer/pipeline_parallel\n",
            "copying apex/transformer/pipeline_parallel/_timers.py -> build/lib/apex/transformer/pipeline_parallel\n",
            "copying apex/transformer/pipeline_parallel/utils.py -> build/lib/apex/transformer/pipeline_parallel\n",
            "copying apex/transformer/pipeline_parallel/p2p_communication.py -> build/lib/apex/transformer/pipeline_parallel\n",
            "copying apex/transformer/pipeline_parallel/__init__.py -> build/lib/apex/transformer/pipeline_parallel\n",
            "creating build/lib/apex/transformer/tensor_parallel\n",
            "copying apex/transformer/tensor_parallel/memory.py -> build/lib/apex/transformer/tensor_parallel\n",
            "copying apex/transformer/tensor_parallel/utils.py -> build/lib/apex/transformer/tensor_parallel\n",
            "copying apex/transformer/tensor_parallel/random.py -> build/lib/apex/transformer/tensor_parallel\n",
            "copying apex/transformer/tensor_parallel/layers.py -> build/lib/apex/transformer/tensor_parallel\n",
            "copying apex/transformer/tensor_parallel/cross_entropy.py -> build/lib/apex/transformer/tensor_parallel\n",
            "copying apex/transformer/tensor_parallel/data.py -> build/lib/apex/transformer/tensor_parallel\n",
            "copying apex/transformer/tensor_parallel/__init__.py -> build/lib/apex/transformer/tensor_parallel\n",
            "copying apex/transformer/tensor_parallel/mappings.py -> build/lib/apex/transformer/tensor_parallel\n",
            "creating build/lib/apex/transformer/_data\n",
            "copying apex/transformer/_data/_batchsampler.py -> build/lib/apex/transformer/_data\n",
            "copying apex/transformer/_data/__init__.py -> build/lib/apex/transformer/_data\n",
            "creating build/lib/apex/transformer/functional\n",
            "copying apex/transformer/functional/__init__.py -> build/lib/apex/transformer/functional\n",
            "copying apex/transformer/functional/fused_softmax.py -> build/lib/apex/transformer/functional\n",
            "copying apex/transformer/functional/fused_rope.py -> build/lib/apex/transformer/functional\n",
            "creating build/lib/apex/transformer/pipeline_parallel/schedules\n",
            "copying apex/transformer/pipeline_parallel/schedules/fwd_bwd_no_pipelining.py -> build/lib/apex/transformer/pipeline_parallel/schedules\n",
            "copying apex/transformer/pipeline_parallel/schedules/__init__.py -> build/lib/apex/transformer/pipeline_parallel/schedules\n",
            "copying apex/transformer/pipeline_parallel/schedules/common.py -> build/lib/apex/transformer/pipeline_parallel/schedules\n",
            "copying apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py -> build/lib/apex/transformer/pipeline_parallel/schedules\n",
            "copying apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py -> build/lib/apex/transformer/pipeline_parallel/schedules\n",
            "creating build/bdist.linux-x86_64\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "creating build/bdist.linux-x86_64/egg/apex\n",
            "copying build/lib/apex/_autocast_utils.py -> build/bdist.linux-x86_64/egg/apex\n",
            "creating build/bdist.linux-x86_64/egg/apex/amp\n",
            "copying build/lib/apex/amp/amp.py -> build/bdist.linux-x86_64/egg/apex/amp\n",
            "copying build/lib/apex/amp/utils.py -> build/bdist.linux-x86_64/egg/apex/amp\n",
            "copying build/lib/apex/amp/compat.py -> build/bdist.linux-x86_64/egg/apex/amp\n",
            "copying build/lib/apex/amp/__version__.py -> build/bdist.linux-x86_64/egg/apex/amp\n",
            "copying build/lib/apex/amp/handle.py -> build/bdist.linux-x86_64/egg/apex/amp\n",
            "copying build/lib/apex/amp/wrap.py -> build/bdist.linux-x86_64/egg/apex/amp\n",
            "copying build/lib/apex/amp/_initialize.py -> build/bdist.linux-x86_64/egg/apex/amp\n",
            "copying build/lib/apex/amp/__init__.py -> build/bdist.linux-x86_64/egg/apex/amp\n",
            "copying build/lib/apex/amp/scaler.py -> build/bdist.linux-x86_64/egg/apex/amp\n",
            "creating build/bdist.linux-x86_64/egg/apex/amp/lists\n",
            "copying build/lib/apex/amp/lists/functional_overrides.py -> build/bdist.linux-x86_64/egg/apex/amp/lists\n",
            "copying build/lib/apex/amp/lists/__init__.py -> build/bdist.linux-x86_64/egg/apex/amp/lists\n",
            "copying build/lib/apex/amp/lists/torch_overrides.py -> build/bdist.linux-x86_64/egg/apex/amp/lists\n",
            "copying build/lib/apex/amp/lists/tensor_overrides.py -> build/bdist.linux-x86_64/egg/apex/amp/lists\n",
            "copying build/lib/apex/amp/frontend.py -> build/bdist.linux-x86_64/egg/apex/amp\n",
            "copying build/lib/apex/amp/_amp_state.py -> build/bdist.linux-x86_64/egg/apex/amp\n",
            "copying build/lib/apex/amp/_process_optimizer.py -> build/bdist.linux-x86_64/egg/apex/amp\n",
            "copying build/lib/apex/amp/rnn_compat.py -> build/bdist.linux-x86_64/egg/apex/amp\n",
            "copying build/lib/apex/amp/opt.py -> build/bdist.linux-x86_64/egg/apex/amp\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib/test\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib/test/group_norm\n",
            "copying build/lib/apex/contrib/test/group_norm/test_group_norm.py -> build/bdist.linux-x86_64/egg/apex/contrib/test/group_norm\n",
            "copying build/lib/apex/contrib/test/group_norm/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/test/group_norm\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib/test/index_mul_2d\n",
            "copying build/lib/apex/contrib/test/index_mul_2d/test_index_mul_2d.py -> build/bdist.linux-x86_64/egg/apex/contrib/test/index_mul_2d\n",
            "copying build/lib/apex/contrib/test/index_mul_2d/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/test/index_mul_2d\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib/test/cudnn_gbn\n",
            "copying build/lib/apex/contrib/test/cudnn_gbn/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/test/cudnn_gbn\n",
            "copying build/lib/apex/contrib/test/cudnn_gbn/test_cudnn_gbn_with_two_gpus.py -> build/bdist.linux-x86_64/egg/apex/contrib/test/cudnn_gbn\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib/test/xentropy\n",
            "copying build/lib/apex/contrib/test/xentropy/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/test/xentropy\n",
            "copying build/lib/apex/contrib/test/xentropy/test_label_smoothing.py -> build/bdist.linux-x86_64/egg/apex/contrib/test/xentropy\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib/test/transducer\n",
            "copying build/lib/apex/contrib/test/transducer/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/test/transducer\n",
            "copying build/lib/apex/contrib/test/transducer/test_transducer_loss.py -> build/bdist.linux-x86_64/egg/apex/contrib/test/transducer\n",
            "copying build/lib/apex/contrib/test/transducer/test_transducer_joint.py -> build/bdist.linux-x86_64/egg/apex/contrib/test/transducer\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib/test/fmha\n",
            "copying build/lib/apex/contrib/test/fmha/test_fmha.py -> build/bdist.linux-x86_64/egg/apex/contrib/test/fmha\n",
            "copying build/lib/apex/contrib/test/fmha/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/test/fmha\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib/test/peer_memory\n",
            "copying build/lib/apex/contrib/test/peer_memory/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/test/peer_memory\n",
            "copying build/lib/apex/contrib/test/peer_memory/test_peer_halo_exchange_module.py -> build/bdist.linux-x86_64/egg/apex/contrib/test/peer_memory\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib/test/conv_bias_relu\n",
            "copying build/lib/apex/contrib/test/conv_bias_relu/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/test/conv_bias_relu\n",
            "copying build/lib/apex/contrib/test/conv_bias_relu/test_conv_bias_relu.py -> build/bdist.linux-x86_64/egg/apex/contrib/test/conv_bias_relu\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib/test/multihead_attn\n",
            "copying build/lib/apex/contrib/test/multihead_attn/test_encdec_multihead_attn_norm_add.py -> build/bdist.linux-x86_64/egg/apex/contrib/test/multihead_attn\n",
            "copying build/lib/apex/contrib/test/multihead_attn/test_self_multihead_attn.py -> build/bdist.linux-x86_64/egg/apex/contrib/test/multihead_attn\n",
            "copying build/lib/apex/contrib/test/multihead_attn/test_self_multihead_attn_norm_add.py -> build/bdist.linux-x86_64/egg/apex/contrib/test/multihead_attn\n",
            "copying build/lib/apex/contrib/test/multihead_attn/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/test/multihead_attn\n",
            "copying build/lib/apex/contrib/test/multihead_attn/test_fast_self_multihead_attn_bias.py -> build/bdist.linux-x86_64/egg/apex/contrib/test/multihead_attn\n",
            "copying build/lib/apex/contrib/test/multihead_attn/test_encdec_multihead_attn.py -> build/bdist.linux-x86_64/egg/apex/contrib/test/multihead_attn\n",
            "copying build/lib/apex/contrib/test/multihead_attn/test_mha_fused_softmax.py -> build/bdist.linux-x86_64/egg/apex/contrib/test/multihead_attn\n",
            "copying build/lib/apex/contrib/test/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/test\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib/test/optimizers\n",
            "copying build/lib/apex/contrib/test/optimizers/test_distributed_fused_lamb.py -> build/bdist.linux-x86_64/egg/apex/contrib/test/optimizers\n",
            "copying build/lib/apex/contrib/test/optimizers/test_dist_adam.py -> build/bdist.linux-x86_64/egg/apex/contrib/test/optimizers\n",
            "copying build/lib/apex/contrib/test/optimizers/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/test/optimizers\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib/test/clip_grad\n",
            "copying build/lib/apex/contrib/test/clip_grad/test_clip_grad.py -> build/bdist.linux-x86_64/egg/apex/contrib/test/clip_grad\n",
            "copying build/lib/apex/contrib/test/clip_grad/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/test/clip_grad\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib/test/bottleneck\n",
            "copying build/lib/apex/contrib/test/bottleneck/test_bottleneck_module.py -> build/bdist.linux-x86_64/egg/apex/contrib/test/bottleneck\n",
            "copying build/lib/apex/contrib/test/bottleneck/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/test/bottleneck\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib/test/focal_loss\n",
            "copying build/lib/apex/contrib/test/focal_loss/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/test/focal_loss\n",
            "copying build/lib/apex/contrib/test/focal_loss/test_focal_loss.py -> build/bdist.linux-x86_64/egg/apex/contrib/test/focal_loss\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib/test/layer_norm\n",
            "copying build/lib/apex/contrib/test/layer_norm/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/test/layer_norm\n",
            "copying build/lib/apex/contrib/test/layer_norm/test_fast_layer_norm.py -> build/bdist.linux-x86_64/egg/apex/contrib/test/layer_norm\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib/group_norm\n",
            "copying build/lib/apex/contrib/group_norm/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/group_norm\n",
            "copying build/lib/apex/contrib/group_norm/group_norm.py -> build/bdist.linux-x86_64/egg/apex/contrib/group_norm\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib/index_mul_2d\n",
            "copying build/lib/apex/contrib/index_mul_2d/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/index_mul_2d\n",
            "copying build/lib/apex/contrib/index_mul_2d/index_mul_2d.py -> build/bdist.linux-x86_64/egg/apex/contrib/index_mul_2d\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib/cudnn_gbn\n",
            "copying build/lib/apex/contrib/cudnn_gbn/batch_norm.py -> build/bdist.linux-x86_64/egg/apex/contrib/cudnn_gbn\n",
            "copying build/lib/apex/contrib/cudnn_gbn/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/cudnn_gbn\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib/openfold_triton\n",
            "copying build/lib/apex/contrib/openfold_triton/mha.py -> build/bdist.linux-x86_64/egg/apex/contrib/openfold_triton\n",
            "copying build/lib/apex/contrib/openfold_triton/_layer_norm_config_hopper.py -> build/bdist.linux-x86_64/egg/apex/contrib/openfold_triton\n",
            "copying build/lib/apex/contrib/openfold_triton/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/openfold_triton\n",
            "copying build/lib/apex/contrib/openfold_triton/_layer_norm_config_ampere.py -> build/bdist.linux-x86_64/egg/apex/contrib/openfold_triton\n",
            "copying build/lib/apex/contrib/openfold_triton/_layer_norm_backward_kernels.py -> build/bdist.linux-x86_64/egg/apex/contrib/openfold_triton\n",
            "copying build/lib/apex/contrib/openfold_triton/fused_adam_swa.py -> build/bdist.linux-x86_64/egg/apex/contrib/openfold_triton\n",
            "copying build/lib/apex/contrib/openfold_triton/_layer_norm_forward_kernels.py -> build/bdist.linux-x86_64/egg/apex/contrib/openfold_triton\n",
            "copying build/lib/apex/contrib/openfold_triton/layer_norm.py -> build/bdist.linux-x86_64/egg/apex/contrib/openfold_triton\n",
            "copying build/lib/apex/contrib/openfold_triton/_mha_kernel.py -> build/bdist.linux-x86_64/egg/apex/contrib/openfold_triton\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib/sparsity\n",
            "copying build/lib/apex/contrib/sparsity/asp.py -> build/bdist.linux-x86_64/egg/apex/contrib/sparsity\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib/sparsity/permutation_search_kernels\n",
            "copying build/lib/apex/contrib/sparsity/permutation_search_kernels/channel_swap.py -> build/bdist.linux-x86_64/egg/apex/contrib/sparsity/permutation_search_kernels\n",
            "copying build/lib/apex/contrib/sparsity/permutation_search_kernels/call_permutation_search_kernels.py -> build/bdist.linux-x86_64/egg/apex/contrib/sparsity/permutation_search_kernels\n",
            "copying build/lib/apex/contrib/sparsity/permutation_search_kernels/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/sparsity/permutation_search_kernels\n",
            "copying build/lib/apex/contrib/sparsity/permutation_search_kernels/permutation_utilities.py -> build/bdist.linux-x86_64/egg/apex/contrib/sparsity/permutation_search_kernels\n",
            "copying build/lib/apex/contrib/sparsity/permutation_search_kernels/exhaustive_search.py -> build/bdist.linux-x86_64/egg/apex/contrib/sparsity/permutation_search_kernels\n",
            "copying build/lib/apex/contrib/sparsity/permutation_lib.py -> build/bdist.linux-x86_64/egg/apex/contrib/sparsity\n",
            "copying build/lib/apex/contrib/sparsity/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/sparsity\n",
            "copying build/lib/apex/contrib/sparsity/sparse_masklib.py -> build/bdist.linux-x86_64/egg/apex/contrib/sparsity\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib/xentropy\n",
            "copying build/lib/apex/contrib/xentropy/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/xentropy\n",
            "copying build/lib/apex/contrib/xentropy/softmax_xentropy.py -> build/bdist.linux-x86_64/egg/apex/contrib/xentropy\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib/transducer\n",
            "copying build/lib/apex/contrib/transducer/_transducer_ref.py -> build/bdist.linux-x86_64/egg/apex/contrib/transducer\n",
            "copying build/lib/apex/contrib/transducer/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/transducer\n",
            "copying build/lib/apex/contrib/transducer/transducer.py -> build/bdist.linux-x86_64/egg/apex/contrib/transducer\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib/fmha\n",
            "copying build/lib/apex/contrib/fmha/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/fmha\n",
            "copying build/lib/apex/contrib/fmha/fmha.py -> build/bdist.linux-x86_64/egg/apex/contrib/fmha\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib/peer_memory\n",
            "copying build/lib/apex/contrib/peer_memory/peer_halo_exchanger_1d.py -> build/bdist.linux-x86_64/egg/apex/contrib/peer_memory\n",
            "copying build/lib/apex/contrib/peer_memory/peer_memory.py -> build/bdist.linux-x86_64/egg/apex/contrib/peer_memory\n",
            "copying build/lib/apex/contrib/peer_memory/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/peer_memory\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib/groupbn\n",
            "copying build/lib/apex/contrib/groupbn/batch_norm.py -> build/bdist.linux-x86_64/egg/apex/contrib/groupbn\n",
            "copying build/lib/apex/contrib/groupbn/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/groupbn\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib/conv_bias_relu\n",
            "copying build/lib/apex/contrib/conv_bias_relu/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/conv_bias_relu\n",
            "copying build/lib/apex/contrib/conv_bias_relu/conv_bias_relu.py -> build/bdist.linux-x86_64/egg/apex/contrib/conv_bias_relu\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn\n",
            "copying build/lib/apex/contrib/multihead_attn/self_multihead_attn_func.py -> build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn\n",
            "copying build/lib/apex/contrib/multihead_attn/encdec_multihead_attn.py -> build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn\n",
            "copying build/lib/apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn\n",
            "copying build/lib/apex/contrib/multihead_attn/self_multihead_attn.py -> build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn\n",
            "copying build/lib/apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn\n",
            "copying build/lib/apex/contrib/multihead_attn/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn\n",
            "copying build/lib/apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn\n",
            "copying build/lib/apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn\n",
            "copying build/lib/apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn\n",
            "copying build/lib/apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn\n",
            "copying build/lib/apex/contrib/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib/gpu_direct_storage\n",
            "copying build/lib/apex/contrib/gpu_direct_storage/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/gpu_direct_storage\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib/optimizers\n",
            "copying build/lib/apex/contrib/optimizers/fused_sgd.py -> build/bdist.linux-x86_64/egg/apex/contrib/optimizers\n",
            "copying build/lib/apex/contrib/optimizers/distributed_fused_lamb.py -> build/bdist.linux-x86_64/egg/apex/contrib/optimizers\n",
            "copying build/lib/apex/contrib/optimizers/distributed_fused_adam.py -> build/bdist.linux-x86_64/egg/apex/contrib/optimizers\n",
            "copying build/lib/apex/contrib/optimizers/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/optimizers\n",
            "copying build/lib/apex/contrib/optimizers/fp16_optimizer.py -> build/bdist.linux-x86_64/egg/apex/contrib/optimizers\n",
            "copying build/lib/apex/contrib/optimizers/fused_lamb.py -> build/bdist.linux-x86_64/egg/apex/contrib/optimizers\n",
            "copying build/lib/apex/contrib/optimizers/fused_adam.py -> build/bdist.linux-x86_64/egg/apex/contrib/optimizers\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib/clip_grad\n",
            "copying build/lib/apex/contrib/clip_grad/clip_grad.py -> build/bdist.linux-x86_64/egg/apex/contrib/clip_grad\n",
            "copying build/lib/apex/contrib/clip_grad/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/clip_grad\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib/bottleneck\n",
            "copying build/lib/apex/contrib/bottleneck/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/bottleneck\n",
            "copying build/lib/apex/contrib/bottleneck/test.py -> build/bdist.linux-x86_64/egg/apex/contrib/bottleneck\n",
            "copying build/lib/apex/contrib/bottleneck/bottleneck.py -> build/bdist.linux-x86_64/egg/apex/contrib/bottleneck\n",
            "copying build/lib/apex/contrib/bottleneck/halo_exchangers.py -> build/bdist.linux-x86_64/egg/apex/contrib/bottleneck\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib/focal_loss\n",
            "copying build/lib/apex/contrib/focal_loss/focal_loss.py -> build/bdist.linux-x86_64/egg/apex/contrib/focal_loss\n",
            "copying build/lib/apex/contrib/focal_loss/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/focal_loss\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib/layer_norm\n",
            "copying build/lib/apex/contrib/layer_norm/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/layer_norm\n",
            "copying build/lib/apex/contrib/layer_norm/layer_norm.py -> build/bdist.linux-x86_64/egg/apex/contrib/layer_norm\n",
            "creating build/bdist.linux-x86_64/egg/apex/RNN\n",
            "copying build/lib/apex/RNN/cells.py -> build/bdist.linux-x86_64/egg/apex/RNN\n",
            "copying build/lib/apex/RNN/models.py -> build/bdist.linux-x86_64/egg/apex/RNN\n",
            "copying build/lib/apex/RNN/__init__.py -> build/bdist.linux-x86_64/egg/apex/RNN\n",
            "copying build/lib/apex/RNN/RNNBackend.py -> build/bdist.linux-x86_64/egg/apex/RNN\n",
            "copying build/lib/apex/__init__.py -> build/bdist.linux-x86_64/egg/apex\n",
            "creating build/bdist.linux-x86_64/egg/apex/parallel\n",
            "copying build/lib/apex/parallel/distributed.py -> build/bdist.linux-x86_64/egg/apex/parallel\n",
            "copying build/lib/apex/parallel/multiproc.py -> build/bdist.linux-x86_64/egg/apex/parallel\n",
            "copying build/lib/apex/parallel/optimized_sync_batchnorm_kernel.py -> build/bdist.linux-x86_64/egg/apex/parallel\n",
            "copying build/lib/apex/parallel/__init__.py -> build/bdist.linux-x86_64/egg/apex/parallel\n",
            "copying build/lib/apex/parallel/sync_batchnorm.py -> build/bdist.linux-x86_64/egg/apex/parallel\n",
            "copying build/lib/apex/parallel/sync_batchnorm_kernel.py -> build/bdist.linux-x86_64/egg/apex/parallel\n",
            "copying build/lib/apex/parallel/LARC.py -> build/bdist.linux-x86_64/egg/apex/parallel\n",
            "copying build/lib/apex/parallel/optimized_sync_batchnorm.py -> build/bdist.linux-x86_64/egg/apex/parallel\n",
            "creating build/bdist.linux-x86_64/egg/apex/optimizers\n",
            "copying build/lib/apex/optimizers/fused_sgd.py -> build/bdist.linux-x86_64/egg/apex/optimizers\n",
            "copying build/lib/apex/optimizers/fused_mixed_precision_lamb.py -> build/bdist.linux-x86_64/egg/apex/optimizers\n",
            "copying build/lib/apex/optimizers/__init__.py -> build/bdist.linux-x86_64/egg/apex/optimizers\n",
            "copying build/lib/apex/optimizers/fused_novograd.py -> build/bdist.linux-x86_64/egg/apex/optimizers\n",
            "copying build/lib/apex/optimizers/fused_adagrad.py -> build/bdist.linux-x86_64/egg/apex/optimizers\n",
            "copying build/lib/apex/optimizers/fused_lamb.py -> build/bdist.linux-x86_64/egg/apex/optimizers\n",
            "copying build/lib/apex/optimizers/fused_adam.py -> build/bdist.linux-x86_64/egg/apex/optimizers\n",
            "creating build/bdist.linux-x86_64/egg/apex/transformer\n",
            "copying build/lib/apex/transformer/log_util.py -> build/bdist.linux-x86_64/egg/apex/transformer\n",
            "copying build/lib/apex/transformer/parallel_state.py -> build/bdist.linux-x86_64/egg/apex/transformer\n",
            "creating build/bdist.linux-x86_64/egg/apex/transformer/layers\n",
            "copying build/lib/apex/transformer/layers/__init__.py -> build/bdist.linux-x86_64/egg/apex/transformer/layers\n",
            "copying build/lib/apex/transformer/layers/layer_norm.py -> build/bdist.linux-x86_64/egg/apex/transformer/layers\n",
            "creating build/bdist.linux-x86_64/egg/apex/transformer/testing\n",
            "copying build/lib/apex/transformer/testing/__init__.py -> build/bdist.linux-x86_64/egg/apex/transformer/testing\n",
            "copying build/lib/apex/transformer/testing/standalone_bert.py -> build/bdist.linux-x86_64/egg/apex/transformer/testing\n",
            "copying build/lib/apex/transformer/testing/distributed_test_base.py -> build/bdist.linux-x86_64/egg/apex/transformer/testing\n",
            "copying build/lib/apex/transformer/testing/arguments.py -> build/bdist.linux-x86_64/egg/apex/transformer/testing\n",
            "copying build/lib/apex/transformer/testing/commons.py -> build/bdist.linux-x86_64/egg/apex/transformer/testing\n",
            "copying build/lib/apex/transformer/testing/global_vars.py -> build/bdist.linux-x86_64/egg/apex/transformer/testing\n",
            "copying build/lib/apex/transformer/testing/standalone_transformer_lm.py -> build/bdist.linux-x86_64/egg/apex/transformer/testing\n",
            "copying build/lib/apex/transformer/testing/standalone_gpt.py -> build/bdist.linux-x86_64/egg/apex/transformer/testing\n",
            "copying build/lib/apex/transformer/utils.py -> build/bdist.linux-x86_64/egg/apex/transformer\n",
            "creating build/bdist.linux-x86_64/egg/apex/transformer/amp\n",
            "copying build/lib/apex/transformer/amp/grad_scaler.py -> build/bdist.linux-x86_64/egg/apex/transformer/amp\n",
            "copying build/lib/apex/transformer/amp/__init__.py -> build/bdist.linux-x86_64/egg/apex/transformer/amp\n",
            "copying build/lib/apex/transformer/enums.py -> build/bdist.linux-x86_64/egg/apex/transformer\n",
            "creating build/bdist.linux-x86_64/egg/apex/transformer/pipeline_parallel\n",
            "copying build/lib/apex/transformer/pipeline_parallel/_timers.py -> build/bdist.linux-x86_64/egg/apex/transformer/pipeline_parallel\n",
            "copying build/lib/apex/transformer/pipeline_parallel/utils.py -> build/bdist.linux-x86_64/egg/apex/transformer/pipeline_parallel\n",
            "creating build/bdist.linux-x86_64/egg/apex/transformer/pipeline_parallel/schedules\n",
            "copying build/lib/apex/transformer/pipeline_parallel/schedules/fwd_bwd_no_pipelining.py -> build/bdist.linux-x86_64/egg/apex/transformer/pipeline_parallel/schedules\n",
            "copying build/lib/apex/transformer/pipeline_parallel/schedules/__init__.py -> build/bdist.linux-x86_64/egg/apex/transformer/pipeline_parallel/schedules\n",
            "copying build/lib/apex/transformer/pipeline_parallel/schedules/common.py -> build/bdist.linux-x86_64/egg/apex/transformer/pipeline_parallel/schedules\n",
            "copying build/lib/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py -> build/bdist.linux-x86_64/egg/apex/transformer/pipeline_parallel/schedules\n",
            "copying build/lib/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py -> build/bdist.linux-x86_64/egg/apex/transformer/pipeline_parallel/schedules\n",
            "copying build/lib/apex/transformer/pipeline_parallel/p2p_communication.py -> build/bdist.linux-x86_64/egg/apex/transformer/pipeline_parallel\n",
            "copying build/lib/apex/transformer/pipeline_parallel/__init__.py -> build/bdist.linux-x86_64/egg/apex/transformer/pipeline_parallel\n",
            "copying build/lib/apex/transformer/__init__.py -> build/bdist.linux-x86_64/egg/apex/transformer\n",
            "copying build/lib/apex/transformer/microbatches.py -> build/bdist.linux-x86_64/egg/apex/transformer\n",
            "creating build/bdist.linux-x86_64/egg/apex/transformer/tensor_parallel\n",
            "copying build/lib/apex/transformer/tensor_parallel/memory.py -> build/bdist.linux-x86_64/egg/apex/transformer/tensor_parallel\n",
            "copying build/lib/apex/transformer/tensor_parallel/utils.py -> build/bdist.linux-x86_64/egg/apex/transformer/tensor_parallel\n",
            "copying build/lib/apex/transformer/tensor_parallel/random.py -> build/bdist.linux-x86_64/egg/apex/transformer/tensor_parallel\n",
            "copying build/lib/apex/transformer/tensor_parallel/layers.py -> build/bdist.linux-x86_64/egg/apex/transformer/tensor_parallel\n",
            "copying build/lib/apex/transformer/tensor_parallel/cross_entropy.py -> build/bdist.linux-x86_64/egg/apex/transformer/tensor_parallel\n",
            "copying build/lib/apex/transformer/tensor_parallel/data.py -> build/bdist.linux-x86_64/egg/apex/transformer/tensor_parallel\n",
            "copying build/lib/apex/transformer/tensor_parallel/__init__.py -> build/bdist.linux-x86_64/egg/apex/transformer/tensor_parallel\n",
            "copying build/lib/apex/transformer/tensor_parallel/mappings.py -> build/bdist.linux-x86_64/egg/apex/transformer/tensor_parallel\n",
            "copying build/lib/apex/transformer/_ucc_util.py -> build/bdist.linux-x86_64/egg/apex/transformer\n",
            "creating build/bdist.linux-x86_64/egg/apex/transformer/_data\n",
            "copying build/lib/apex/transformer/_data/_batchsampler.py -> build/bdist.linux-x86_64/egg/apex/transformer/_data\n",
            "copying build/lib/apex/transformer/_data/__init__.py -> build/bdist.linux-x86_64/egg/apex/transformer/_data\n",
            "creating build/bdist.linux-x86_64/egg/apex/transformer/functional\n",
            "copying build/lib/apex/transformer/functional/__init__.py -> build/bdist.linux-x86_64/egg/apex/transformer/functional\n",
            "copying build/lib/apex/transformer/functional/fused_softmax.py -> build/bdist.linux-x86_64/egg/apex/transformer/functional\n",
            "copying build/lib/apex/transformer/functional/fused_rope.py -> build/bdist.linux-x86_64/egg/apex/transformer/functional\n",
            "creating build/bdist.linux-x86_64/egg/apex/multi_tensor_apply\n",
            "copying build/lib/apex/multi_tensor_apply/__init__.py -> build/bdist.linux-x86_64/egg/apex/multi_tensor_apply\n",
            "copying build/lib/apex/multi_tensor_apply/multi_tensor_apply.py -> build/bdist.linux-x86_64/egg/apex/multi_tensor_apply\n",
            "creating build/bdist.linux-x86_64/egg/apex/fp16_utils\n",
            "copying build/lib/apex/fp16_utils/fp16util.py -> build/bdist.linux-x86_64/egg/apex/fp16_utils\n",
            "copying build/lib/apex/fp16_utils/loss_scaler.py -> build/bdist.linux-x86_64/egg/apex/fp16_utils\n",
            "copying build/lib/apex/fp16_utils/__init__.py -> build/bdist.linux-x86_64/egg/apex/fp16_utils\n",
            "copying build/lib/apex/fp16_utils/fp16_optimizer.py -> build/bdist.linux-x86_64/egg/apex/fp16_utils\n",
            "creating build/bdist.linux-x86_64/egg/apex/mlp\n",
            "copying build/lib/apex/mlp/mlp.py -> build/bdist.linux-x86_64/egg/apex/mlp\n",
            "copying build/lib/apex/mlp/__init__.py -> build/bdist.linux-x86_64/egg/apex/mlp\n",
            "creating build/bdist.linux-x86_64/egg/apex/fused_dense\n",
            "copying build/lib/apex/fused_dense/__init__.py -> build/bdist.linux-x86_64/egg/apex/fused_dense\n",
            "copying build/lib/apex/fused_dense/fused_dense.py -> build/bdist.linux-x86_64/egg/apex/fused_dense\n",
            "creating build/bdist.linux-x86_64/egg/apex/normalization\n",
            "copying build/lib/apex/normalization/__init__.py -> build/bdist.linux-x86_64/egg/apex/normalization\n",
            "copying build/lib/apex/normalization/fused_layer_norm.py -> build/bdist.linux-x86_64/egg/apex/normalization\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/_autocast_utils.py to _autocast_utils.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/amp/amp.py to amp.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/amp/utils.py to utils.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/amp/compat.py to compat.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/amp/__version__.py to __version__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/amp/handle.py to handle.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/amp/wrap.py to wrap.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/amp/_initialize.py to _initialize.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/amp/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/amp/scaler.py to scaler.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/amp/lists/functional_overrides.py to functional_overrides.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/amp/lists/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/amp/lists/torch_overrides.py to torch_overrides.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/amp/lists/tensor_overrides.py to tensor_overrides.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/amp/frontend.py to frontend.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/amp/_amp_state.py to _amp_state.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/amp/_process_optimizer.py to _process_optimizer.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/amp/rnn_compat.py to rnn_compat.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/amp/opt.py to opt.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/test/group_norm/test_group_norm.py to test_group_norm.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/test/group_norm/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/test/index_mul_2d/test_index_mul_2d.py to test_index_mul_2d.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/test/index_mul_2d/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/test/cudnn_gbn/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/test/cudnn_gbn/test_cudnn_gbn_with_two_gpus.py to test_cudnn_gbn_with_two_gpus.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/test/xentropy/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/test/xentropy/test_label_smoothing.py to test_label_smoothing.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/test/transducer/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/test/transducer/test_transducer_loss.py to test_transducer_loss.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/test/transducer/test_transducer_joint.py to test_transducer_joint.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/test/fmha/test_fmha.py to test_fmha.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/test/fmha/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/test/peer_memory/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/test/peer_memory/test_peer_halo_exchange_module.py to test_peer_halo_exchange_module.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/test/conv_bias_relu/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/test/conv_bias_relu/test_conv_bias_relu.py to test_conv_bias_relu.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/test/multihead_attn/test_encdec_multihead_attn_norm_add.py to test_encdec_multihead_attn_norm_add.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/test/multihead_attn/test_self_multihead_attn.py to test_self_multihead_attn.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/test/multihead_attn/test_self_multihead_attn_norm_add.py to test_self_multihead_attn_norm_add.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/test/multihead_attn/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/test/multihead_attn/test_fast_self_multihead_attn_bias.py to test_fast_self_multihead_attn_bias.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/test/multihead_attn/test_encdec_multihead_attn.py to test_encdec_multihead_attn.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/test/multihead_attn/test_mha_fused_softmax.py to test_mha_fused_softmax.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/test/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/test/optimizers/test_distributed_fused_lamb.py to test_distributed_fused_lamb.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/test/optimizers/test_dist_adam.py to test_dist_adam.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/test/optimizers/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/test/clip_grad/test_clip_grad.py to test_clip_grad.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/test/clip_grad/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/test/bottleneck/test_bottleneck_module.py to test_bottleneck_module.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/test/bottleneck/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/test/focal_loss/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/test/focal_loss/test_focal_loss.py to test_focal_loss.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/test/layer_norm/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/test/layer_norm/test_fast_layer_norm.py to test_fast_layer_norm.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/group_norm/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/group_norm/group_norm.py to group_norm.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/index_mul_2d/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/index_mul_2d/index_mul_2d.py to index_mul_2d.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/cudnn_gbn/batch_norm.py to batch_norm.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/cudnn_gbn/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/openfold_triton/mha.py to mha.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/openfold_triton/_layer_norm_config_hopper.py to _layer_norm_config_hopper.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/openfold_triton/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/openfold_triton/_layer_norm_config_ampere.py to _layer_norm_config_ampere.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/openfold_triton/_layer_norm_backward_kernels.py to _layer_norm_backward_kernels.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/openfold_triton/fused_adam_swa.py to fused_adam_swa.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/openfold_triton/_layer_norm_forward_kernels.py to _layer_norm_forward_kernels.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/openfold_triton/layer_norm.py to layer_norm.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/openfold_triton/_mha_kernel.py to _mha_kernel.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/sparsity/asp.py to asp.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/sparsity/permutation_search_kernels/channel_swap.py to channel_swap.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/sparsity/permutation_search_kernels/call_permutation_search_kernels.py to call_permutation_search_kernels.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/sparsity/permutation_search_kernels/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/sparsity/permutation_search_kernels/permutation_utilities.py to permutation_utilities.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/sparsity/permutation_search_kernels/exhaustive_search.py to exhaustive_search.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/sparsity/permutation_lib.py to permutation_lib.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/sparsity/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/sparsity/sparse_masklib.py to sparse_masklib.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/xentropy/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/xentropy/softmax_xentropy.py to softmax_xentropy.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/transducer/_transducer_ref.py to _transducer_ref.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/transducer/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/transducer/transducer.py to transducer.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/fmha/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/fmha/fmha.py to fmha.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/peer_memory/peer_halo_exchanger_1d.py to peer_halo_exchanger_1d.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/peer_memory/peer_memory.py to peer_memory.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/peer_memory/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/groupbn/batch_norm.py to batch_norm.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/groupbn/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/conv_bias_relu/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/conv_bias_relu/conv_bias_relu.py to conv_bias_relu.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn/self_multihead_attn_func.py to self_multihead_attn_func.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn/encdec_multihead_attn.py to encdec_multihead_attn.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn/fast_self_multihead_attn_func.py to fast_self_multihead_attn_func.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn/self_multihead_attn.py to self_multihead_attn.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn/mask_softmax_dropout_func.py to mask_softmax_dropout_func.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py to fast_self_multihead_attn_norm_add_func.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn/encdec_multihead_attn_func.py to encdec_multihead_attn_func.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py to fast_encdec_multihead_attn_norm_add_func.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py to fast_encdec_multihead_attn_func.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/gpu_direct_storage/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/optimizers/fused_sgd.py to fused_sgd.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/optimizers/distributed_fused_lamb.py to distributed_fused_lamb.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/optimizers/distributed_fused_adam.py to distributed_fused_adam.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/optimizers/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/optimizers/fp16_optimizer.py to fp16_optimizer.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/optimizers/fused_lamb.py to fused_lamb.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/optimizers/fused_adam.py to fused_adam.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/clip_grad/clip_grad.py to clip_grad.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/clip_grad/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/bottleneck/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/bottleneck/test.py to test.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/bottleneck/bottleneck.py to bottleneck.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/bottleneck/halo_exchangers.py to halo_exchangers.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/focal_loss/focal_loss.py to focal_loss.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/focal_loss/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/layer_norm/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/layer_norm/layer_norm.py to layer_norm.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/RNN/cells.py to cells.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/RNN/models.py to models.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/RNN/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/RNN/RNNBackend.py to RNNBackend.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/parallel/distributed.py to distributed.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/parallel/multiproc.py to multiproc.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/parallel/optimized_sync_batchnorm_kernel.py to optimized_sync_batchnorm_kernel.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/parallel/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/parallel/sync_batchnorm.py to sync_batchnorm.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/parallel/sync_batchnorm_kernel.py to sync_batchnorm_kernel.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/parallel/LARC.py to LARC.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/parallel/optimized_sync_batchnorm.py to optimized_sync_batchnorm.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/optimizers/fused_sgd.py to fused_sgd.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/optimizers/fused_mixed_precision_lamb.py to fused_mixed_precision_lamb.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/optimizers/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/optimizers/fused_novograd.py to fused_novograd.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/optimizers/fused_adagrad.py to fused_adagrad.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/optimizers/fused_lamb.py to fused_lamb.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/optimizers/fused_adam.py to fused_adam.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/log_util.py to log_util.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/parallel_state.py to parallel_state.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/layers/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/layers/layer_norm.py to layer_norm.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/testing/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/testing/standalone_bert.py to standalone_bert.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/testing/distributed_test_base.py to distributed_test_base.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/testing/arguments.py to arguments.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/testing/commons.py to commons.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/testing/global_vars.py to global_vars.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/testing/standalone_transformer_lm.py to standalone_transformer_lm.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/testing/standalone_gpt.py to standalone_gpt.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/utils.py to utils.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/amp/grad_scaler.py to grad_scaler.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/amp/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/enums.py to enums.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/pipeline_parallel/_timers.py to _timers.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/pipeline_parallel/utils.py to utils.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/pipeline_parallel/schedules/fwd_bwd_no_pipelining.py to fwd_bwd_no_pipelining.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/pipeline_parallel/schedules/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/pipeline_parallel/schedules/common.py to common.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py to fwd_bwd_pipelining_with_interleaving.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py to fwd_bwd_pipelining_without_interleaving.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/pipeline_parallel/p2p_communication.py to p2p_communication.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/pipeline_parallel/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/microbatches.py to microbatches.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/tensor_parallel/memory.py to memory.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/tensor_parallel/utils.py to utils.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/tensor_parallel/random.py to random.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/tensor_parallel/layers.py to layers.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/tensor_parallel/cross_entropy.py to cross_entropy.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/tensor_parallel/data.py to data.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/tensor_parallel/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/tensor_parallel/mappings.py to mappings.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/_ucc_util.py to _ucc_util.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/_data/_batchsampler.py to _batchsampler.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/_data/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/functional/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/functional/fused_softmax.py to fused_softmax.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/functional/fused_rope.py to fused_rope.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/multi_tensor_apply/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/multi_tensor_apply/multi_tensor_apply.py to multi_tensor_apply.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/fp16_utils/fp16util.py to fp16util.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/fp16_utils/loss_scaler.py to loss_scaler.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/fp16_utils/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/fp16_utils/fp16_optimizer.py to fp16_optimizer.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/mlp/mlp.py to mlp.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/mlp/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/fused_dense/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/fused_dense/fused_dense.py to fused_dense.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/normalization/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/normalization/fused_layer_norm.py to fused_layer_norm.cpython-310.pyc\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying apex.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying apex.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying apex.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying apex.egg-info/requires.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying apex.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "apex.contrib.optimizers.__pycache__.distributed_fused_adam.cpython-310: module MAY be using inspect.stack\n",
            "creating dist\n",
            "creating 'dist/apex-0.1-py3.10.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing apex-0.1-py3.10.egg\n",
            "creating /usr/local/lib/python3.10/dist-packages/apex-0.1-py3.10.egg\n",
            "Extracting apex-0.1-py3.10.egg to /usr/local/lib/python3.10/dist-packages\n",
            "Adding apex 0.1 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.10/dist-packages/apex-0.1-py3.10.egg\n",
            "Processing dependencies for apex==0.1\n",
            "Searching for packaging==23.2\n",
            "Best match: packaging 23.2\n",
            "Adding packaging 23.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Finished processing dependencies for apex==0.1\n"
          ]
        }
      ],
      "source": [
        "!pip -q install datasets apex pyramid tokenizers\n",
        "!git clone https://github.com/NVIDIA/apex\n",
        "%cd apex\n",
        "!python3 setup.py install\n",
        "import numpy as np\n",
        "from transformers import BertTokenizer, BertConfig, BertModel\n",
        "from datasets import load_dataset\n",
        "import random\n",
        "import pyramid\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from scipy.sparse import csr_matrix, csc_matrix, vstack\n",
        "from tqdm import tqdm\n",
        "from tokenizers import BertWordPieceTokenizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "from transformers import AdamW\n",
        "from apex import amp\n",
        "from torch import nn\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "EhqOdsV5xKNB"
      },
      "outputs": [],
      "source": [
        "#VARS\n",
        "\n",
        "levels = []\n",
        "eps = 1e-4\n",
        "eval_step = 20000\n",
        "seed = 42\n",
        "max_leaf = 20\n",
        "BATCH = 16\n",
        "EPOCH = 20\n",
        "max_len = 512\n",
        "group_y_candidate_num = 3000\n",
        "swa_warmup = 10\n",
        "swa_step = 100\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "labels = []\n",
        "texts = []\n",
        "dataType = []\n",
        "label_map = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YqdqUgYvwiJE",
        "outputId": "eb7e4b2f-ead8-4c3f-9781-b0e4248d8d0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "15449it [00:04, 3475.03it/s]\n",
            "3865it [00:01, 2338.88it/s]\n",
            "15449it [00:00, 23794.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3801\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "3865it [00:00, 4546.93it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3956\n",
            "3956\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "if (os.path.exists('/content/drive') == 0):\n",
        "  drive.mount('/content/drive')\n",
        "# drive_contents = os.listdir('/content/drive/MyDrive/EUR-Lex')\n",
        "# print(drive_contents)\n",
        "fext = '_texts.txt'\n",
        "with open(f'/content/drive/MyDrive/EUR-Lex/train{fext}') as f:\n",
        "    for i in tqdm(f):\n",
        "        texts.append(i.replace('\\n', ''))\n",
        "        dataType.append('train')\n",
        "\n",
        "with open(f'/content/drive/MyDrive/EUR-Lex/test{fext}') as f:\n",
        "    for i in tqdm(f):\n",
        "        texts.append(i.replace('\\n', ''))\n",
        "        dataType.append('test')\n",
        "\n",
        "with open(f'/content/drive/MyDrive/EUR-Lex/train_labels.txt') as f:\n",
        "    for i in tqdm(f):\n",
        "        for l in i.replace('\\n', '').split():\n",
        "            label_map[l] = 0\n",
        "        labels.append(i.replace('\\n', ''))\n",
        "\n",
        "\n",
        "with open(f'/content/drive/MyDrive/EUR-Lex/test_labels.txt') as f:\n",
        "    print(len(label_map))\n",
        "    for i in tqdm(f):\n",
        "        for l in i.replace('\\n', '').split():\n",
        "            label_map[l] = 0\n",
        "        labels.append(i.replace('\\n', ''))\n",
        "    print(len(label_map))\n",
        "\n",
        "assert len(texts) == len(labels) == len(dataType)\n",
        "\n",
        "df_row = {'text': texts, 'label': labels, 'dataType': dataType}\n",
        "\n",
        "for i, k in enumerate(sorted(label_map.keys())):\n",
        "    label_map[k] = i\n",
        "df = pd.DataFrame(df_row)\n",
        "print(len(label_map))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "-Ph3JnnFzwVv"
      },
      "outputs": [],
      "source": [
        "class MDataset(Dataset):\n",
        "    def __init__(self, df, mode, tokenizer, label_map, max_length,\n",
        "                 token_type_ids=None, group_y=None, candidates_num=None):\n",
        "        assert mode in [\"train\", \"valid\", \"test\"]\n",
        "        self.mode = mode\n",
        "        self.df, self.n_labels, self.label_map = df[df.dataType == self.mode], len(label_map), label_map\n",
        "        self.len = len(self.df)\n",
        "        self.tokenizer, self.max_length, self.group_y = tokenizer, max_length, group_y\n",
        "        self.multi_group = False\n",
        "        self.token_type_ids = token_type_ids\n",
        "        self.candidates_num = candidates_num\n",
        "\n",
        "        if group_y is not None:\n",
        "            # print(\"GROUP Y\")\n",
        "            # print(group_y)\n",
        "            # group y mode\n",
        "            self.candidates_num, self.group_y, self.n_group_y_labels = candidates_num, [], group_y.shape[0]\n",
        "            # print(len(label_map))\n",
        "            self.map_group_y = np.empty(len(label_map), dtype=np.compat.long)\n",
        "            # ctr = 0\n",
        "            for idx, labels in enumerate(group_y):\n",
        "                self.group_y.append([])\n",
        "                for label in labels:\n",
        "                    # ctr += 1\n",
        "                    self.group_y[-1].append(label_map[label])\n",
        "                self.map_group_y[self.group_y[-1]] = idx\n",
        "                self.group_y[-1]  = np.array(self.group_y[-1],dtype=int)\n",
        "            # print(self.map_group_y)\n",
        "            # assert ctr == 3956\n",
        "            self.group_y = np.array(self.group_y,dtype=object)\n",
        "        else:\n",
        "          self.group_y = None\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # print(idx)\n",
        "        # print(\"here\")\n",
        "        # print(self.group_y == None)\n",
        "        max_len = self.max_length\n",
        "        review = self.df.text.values[idx].lower()\n",
        "        labels = [self.label_map[i] for i in self.df.label.values[idx].split() if i in self.label_map]\n",
        "        # print(\"len of labels is \" + str(len(labels)))\n",
        "        review = ' '.join(review.split()[:max_len])\n",
        "\n",
        "        text = review\n",
        "        if self.token_type_ids is not None:\n",
        "            input_ids = self.token_type_ids[idx]\n",
        "            if input_ids[-1] == 0:\n",
        "                input_ids = input_ids[input_ids != 0]\n",
        "            input_ids = input_ids.tolist()\n",
        "        elif hasattr(self.tokenizer, 'encode_plus'):\n",
        "            input_ids = self.tokenizer.encode(\n",
        "                'filling empty' if len(text) == 0 else text,\n",
        "                add_special_tokens=True,\n",
        "                max_length=max_len\n",
        "            )\n",
        "        else:\n",
        "            # fast\n",
        "            input_ids = self.tokenizer.encode(\n",
        "                'filling empty' if len(text) == 0 else text,\n",
        "                add_special_tokens=True\n",
        "            ).ids\n",
        "\n",
        "        if len(input_ids) == 0:\n",
        "            print('zero string')\n",
        "            assert 0\n",
        "        if len(input_ids) > self.max_length:\n",
        "            input_ids[self.max_length-1] = input_ids[-1]\n",
        "            input_ids = input_ids[:self.max_length]\n",
        "\n",
        "        attention_mask = [1] * len(input_ids)\n",
        "        token_type_ids = [0] * len(input_ids)\n",
        "\n",
        "        padding_length = self.max_length - len(input_ids)\n",
        "        input_ids = input_ids + ([0] * padding_length)\n",
        "        attention_mask = attention_mask + ([0] * padding_length)\n",
        "        token_type_ids = token_type_ids + ([0] * padding_length)\n",
        "\n",
        "        input_ids = torch.tensor(input_ids)\n",
        "        attention_mask = torch.tensor(attention_mask)\n",
        "        token_type_ids = torch.tensor(token_type_ids)\n",
        "\n",
        "\n",
        "        if self.group_y is not None:\n",
        "            # print(\"DAAAAMN\")\n",
        "            label_ids = torch.zeros(self.n_labels)\n",
        "            label_ids = label_ids.scatter(0, torch.tensor(labels),\n",
        "                                          torch.tensor([1.0 for i in labels]))\n",
        "            group_labels = self.map_group_y[labels].tolist()\n",
        "            group_labels = [label for label in group_labels if label < self.n_group_y_labels]\n",
        "            # print(self.map_group_y)\n",
        "            if self.multi_group:\n",
        "                group_labels = np.concatenate(group_labels).tolist()\n",
        "            # print(group_labels)\n",
        "            group_label_ids = torch.zeros(self.n_group_y_labels)\n",
        "            group_label_ids = group_label_ids.scatter(0, torch.tensor(group_labels),\n",
        "                                      torch.tensor([1.0 for i in group_labels]))\n",
        "            # except Exception as e:\n",
        "            #   print(group_labels)\n",
        "            candidates = np.concatenate(self.group_y[group_labels], axis=0)\n",
        "\n",
        "            if len(candidates) < self.candidates_num:\n",
        "                sample = np.random.randint(self.n_group_y_labels, size=self.candidates_num - len(candidates))\n",
        "                candidates = np.concatenate([candidates, sample])\n",
        "            elif len(candidates) > self.candidates_num:\n",
        "                candidates = np.random.choice(candidates, self.candidates_num, replace=False)\n",
        "\n",
        "            if self.mode == 'train':\n",
        "                return input_ids, attention_mask, token_type_ids,\\\n",
        "                    label_ids[candidates], group_label_ids, candidates\n",
        "            else:\n",
        "                return input_ids, attention_mask, token_type_ids,\\\n",
        "                    label_ids, group_label_ids, candidates\n",
        "            # root fc layer\n",
        "            group_labels = layers_group_labels[0]\n",
        "            group_label_ids = torch.zeros(len(self.map_children[0]))\n",
        "            group_label_ids = group_label_ids.scatter(0, torch.tensor(group_labels),\n",
        "                                                      torch.tensor([1.0 for i in group_labels]))\n",
        "            layers_group_labels_ids.append(group_label_ids)\n",
        "\n",
        "            if self.mode == 'train':\n",
        "                return input_ids, attention_mask, token_type_ids, \\\n",
        "                    layers_group_labels_ids[::-1], layers_candidates[::-1]\n",
        "            else:\n",
        "                return input_ids, attention_mask, token_type_ids, layers_group_labels + [labels]\n",
        "\n",
        "        label_ids = torch.zeros(self.n_labels)\n",
        "        label_ids = label_ids.scatter(0, torch.tensor(labels),\n",
        "                                      torch.tensor([1.0 for i in labels]))\n",
        "        return input_ids, attention_mask, token_type_ids, label_ids\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "nMrozO8ax-CC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141,
          "referenced_widgets": [
            "1f0fb671d7a8498aba4d12f7ad65bb32",
            "d4e380b1a16941ffb9feda018542e17c",
            "f8517e8bc2ae44c4a97d487477f988d4",
            "dee7bfaf69474153b6d6d7a9a2887df4",
            "3fd5b2c8702c45e2bd9ab8b45c5aa784"
          ]
        },
        "outputId": "0da69c0e-a14c-44d3-c5c1-bc2aa072491e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1f0fb671d7a8498aba4d12f7ad65bb32"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d4e380b1a16941ffb9feda018542e17c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f8517e8bc2ae44c4a97d487477f988d4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dee7bfaf69474153b6d6d7a9a2887df4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3fd5b2c8702c45e2bd9ab8b45c5aa784"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "model_config = BertConfig.from_pretrained('bert-base-uncased')\n",
        "model_config.output_hidden_states = True\n",
        "model_bert = BertModel.from_pretrained('bert-base-uncased', config=model_config)\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "854lJYjNy814"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_svmlight_file\n",
        "def build_tree_by_level(eps: float, max_leaf: int, levels: list):\n",
        "  print('Clustering')\n",
        "  fext = '.txt'\n",
        "  sparse_x_train, _ = load_svmlight_file(f'/content/drive/MyDrive/EUR-Lex/train{fext}', multilabel=True)\n",
        "  sparse_labels_train = [i.replace('\\n', '').split() for i in open('/content/drive/MyDrive/EUR-Lex/train_labels.txt')]\n",
        "  sparse_x_test, _ = load_svmlight_file(f'/content/drive/MyDrive/EUR-Lex/test{fext}', multilabel=True)\n",
        "  sparse_labels_test = [i.replace('\\n', '').split() for i in open('/content/drive/MyDrive/EUR-Lex/test_labels.txt')]\n",
        "  sparse_labels = sparse_labels_train + sparse_labels_test\n",
        "  sparse_x = vstack([sparse_x_train, sparse_x_test])\n",
        "  sparse_x, sparse_y = normalize(sparse_x), np.array(sparse_labels,dtype=object)\n",
        "  mlb = MultiLabelBinarizer(sparse_output=True)\n",
        "  sparse_y = mlb.fit_transform(sparse_y)\n",
        "  # sparse_x, sparse_labels = get_sparse_feature(sparse_data_x, sparse_data_y)\n",
        "  # mlb = MultiLabelBinarizer(sparse_output=True)\n",
        "  # sparse_y = mlb.fit_transform(sparse_labels)\n",
        "  # joblib.dump(mlb, groups_path+'mlb')\n",
        "  print('Getting Labels Feature')\n",
        "  labels_f = normalize(csr_matrix(sparse_y.T) @ csc_matrix(sparse_x))\n",
        "  print(F'Start Clustering {levels}')\n",
        "  levels, q = [2**x for x in levels], None\n",
        "      # if os.path.exists(F'{groups_path}-Level-{i}.npy'):\n",
        "      #     print(F'{groups_path}-Level-{i}.npy')\n",
        "      #     labels_list = np.load(F'{groups_path}-Level-{i}.npy', allow_pickle=True)\n",
        "      #     q = [(labels_i, labels_f[labels_i]) for labels_i in labels_list]\n",
        "      #     break\n",
        "  if q is None:\n",
        "    q = [(np.arange(labels_f.shape[0]), labels_f)]\n",
        "  # print(q)\n",
        "  while q:\n",
        "    labels_list = np.asarray([x[0] for x in q],dtype=object)\n",
        "    assert sum(len(labels) for labels in labels_list) == labels_f.shape[0]\n",
        "    # print(len(labels_list))\n",
        "    # print(labels_list)\n",
        "    # break\n",
        "    if len(labels_list) in levels:\n",
        "        level = levels.index(len(labels_list))\n",
        "        print(F'Finish Clustering Level-{level}')\n",
        "        # np.save(F'{groups_path}-Level-{level}.npy', np.asarray(labels_list))\n",
        "    else:\n",
        "        print(F'Finish Clustering {len(labels_list)}')\n",
        "    next_q = []\n",
        "    for node_i, node_f in q:\n",
        "        # print(len(node_i))\n",
        "        # print(len(node_f))\n",
        "        # print(node_i)\n",
        "        # break\n",
        "        if len(node_i) > max_leaf:\n",
        "            next_q += list(split_node(node_i, node_f, eps))\n",
        "        else:\n",
        "          np.save('last.npy', np.asarray(labels_list))\n",
        "    q = next_q\n",
        "  print('Finish Clustering')\n",
        "  # print(labels_list)\n",
        "  # print(len(labels_list))\n",
        "  # for i in labels_list:\n",
        "  #   # print(len(i))\n",
        "  return mlb, np.asarray(labels_list,dtype=object)\n",
        "\n",
        "def split_node(labels_i: np.ndarray, labels_f: csr_matrix, eps: float):\n",
        "  n = len(labels_i)\n",
        "  c1, c2 = np.random.choice(np.arange(n), 2, replace=False)\n",
        "  centers, old_dis, new_dis = labels_f[[c1, c2]].toarray(), -10000.0, -1.0\n",
        "  l_labels_i, r_labels_i = None, None\n",
        "  while new_dis - old_dis >= eps:\n",
        "      dis = labels_f @ centers.T  # N, 2\n",
        "      partition = np.argsort(dis[:, 1] - dis[:, 0])\n",
        "      l_labels_i, r_labels_i = partition[:n//2], partition[n//2:]\n",
        "      old_dis, new_dis = new_dis, (dis[l_labels_i, 0].sum() + dis[r_labels_i, 1].sum()) / n\n",
        "      centers = normalize(np.asarray([np.squeeze(np.asarray(labels_f[l_labels_i].sum(axis=0))),\n",
        "                                      np.squeeze(np.asarray(labels_f[r_labels_i].sum(axis=0)))]))\n",
        "  return (labels_i[l_labels_i], labels_f[l_labels_i]), (labels_i[r_labels_i], labels_f[r_labels_i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4G3rLA4YnhyT",
        "outputId": "53048acd-ce7f-4665-b156-a3e7bf5ef6e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clustering\n",
            "Getting Labels Feature\n",
            "Start Clustering []\n",
            "Finish Clustering 1\n",
            "Finish Clustering 2\n",
            "Finish Clustering 4\n",
            "Finish Clustering 8\n",
            "Finish Clustering 16\n",
            "Finish Clustering 32\n",
            "Finish Clustering 64\n",
            "Finish Clustering 128\n",
            "Finish Clustering 256\n",
            "Finish Clustering\n"
          ]
        }
      ],
      "source": [
        "mlb, groups = build_tree_by_level(eps, max_leaf, levels)\n",
        "groups = np.load('last.npy', allow_pickle=True)\n",
        "new_group = []\n",
        "for group in groups:\n",
        "  new_group.append([mlb.classes_[i] for i in group])\n",
        "new_group = np.array(new_group,dtype=object)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ivkSdrO2v-_4"
      },
      "outputs": [],
      "source": [
        "# candidates_num, group_y, n_group_y_labels = 3000, [], new_group.shape[0]\n",
        "# map_group_y = np.empty(len(label_map), dtype=np.compat.long)\n",
        "# for idx, labels in enumerate(group_y):\n",
        "#     group_y.append([])\n",
        "#     for label in labels:\n",
        "#         group_y[-1].append(label_map[label])\n",
        "#     map_group_y[group_y[-1]] = idx\n",
        "#     group_y[-1]  = np.array(group_y[-1])\n",
        "# group_y = np.array(group_y)\n",
        "# print(map_group_y)\n",
        "# print(group_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "JfTTboEf2qwk"
      },
      "outputs": [],
      "source": [
        "class LightXML(nn.Module):\n",
        "    def __init__(self,model , n_labels, group_y=None, bert='bert-base', feature_layers=5, dropout=0.5, update_count=1,\n",
        "                 candidates_topk=10,\n",
        "                 use_swa=True, swa_warmup_epoch=10, swa_update_step=200, hidden_dim=300):\n",
        "        super(LightXML, self).__init__()\n",
        "\n",
        "        self.use_swa = use_swa\n",
        "        self.swa_warmup_epoch = swa_warmup_epoch\n",
        "        self.swa_update_step = swa_update_step\n",
        "        self.swa_state = {}\n",
        "\n",
        "        self.update_count = update_count\n",
        "\n",
        "        self.candidates_topk = candidates_topk\n",
        "\n",
        "        print('swa', self.use_swa, self.swa_warmup_epoch, self.swa_update_step, self.swa_state)\n",
        "        print('update_count', self.update_count)\n",
        "\n",
        "        self.bert_name, self.bert = bert, model\n",
        "        self.feature_layers, self.drop_out = feature_layers, nn.Dropout(dropout)\n",
        "\n",
        "        self.group_y = group_y\n",
        "        if self.group_y is not None:\n",
        "            self.group_y_labels = group_y.shape[0]\n",
        "            print('hidden dim:',  hidden_dim)\n",
        "            print('label goup numbers:',  self.group_y_labels)\n",
        "\n",
        "            self.l0 = nn.Linear(self.feature_layers*self.bert.config.hidden_size, self.group_y_labels)\n",
        "            # hidden bottle layer\n",
        "            self.l1 = nn.Linear(self.feature_layers*self.bert.config.hidden_size, hidden_dim)\n",
        "            self.embed = nn.Embedding(n_labels, hidden_dim)\n",
        "            nn.init.xavier_uniform_(self.embed.weight)\n",
        "        else:\n",
        "            self.l0 = nn.Linear(self.feature_layers*self.bert.config.hidden_size, n_labels)\n",
        "\n",
        "    def get_candidates(self, group_logits, group_gd=None):\n",
        "        logits = torch.sigmoid(group_logits.detach())\n",
        "        if group_gd is not None:\n",
        "            logits += group_gd\n",
        "        scores, indices = torch.topk(logits, k=self.candidates_topk)\n",
        "        scores, indices = scores.cpu().detach().numpy(), indices.cpu().detach().numpy()\n",
        "        candidates, candidates_scores = [], []\n",
        "        for index, score in zip(indices, scores):\n",
        "            candidates.append(self.group_y[index])\n",
        "            candidates_scores.append([np.full(c.shape, s) for c, s in zip(candidates[-1], score)])\n",
        "            candidates[-1] = np.concatenate(candidates[-1])\n",
        "            candidates_scores[-1] = np.concatenate(candidates_scores[-1])\n",
        "        max_candidates = max([i.shape[0] for i in candidates])\n",
        "        candidates = np.stack([np.pad(i, (0, max_candidates - i.shape[0]), mode='edge') for i in candidates])\n",
        "        candidates_scores = np.stack([np.pad(i, (0, max_candidates - i.shape[0]), mode='edge') for i in candidates_scores])\n",
        "        return indices, candidates, candidates_scores\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, token_type_ids,\n",
        "                labels=None, group_labels=None, candidates=None):\n",
        "        is_training = labels is not None\n",
        "\n",
        "        outs = self.bert(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids\n",
        "        )[-1]\n",
        "\n",
        "        out = torch.cat([outs[-i][:, 0] for i in range(1, self.feature_layers+1)], dim=-1)\n",
        "        out = self.drop_out(out)\n",
        "        group_logits = self.l0(out)\n",
        "        if self.group_y is None:\n",
        "            logits = group_logits\n",
        "            if is_training:\n",
        "                loss_fn = torch.nn.BCEWithLogitsLoss()\n",
        "                loss = loss_fn(logits, labels)\n",
        "                return logits, loss\n",
        "            else:\n",
        "                return logits\n",
        "\n",
        "        if is_training:\n",
        "            l = labels.to(dtype=torch.bool)\n",
        "            target_candidates = torch.masked_select(candidates, l).detach().cpu()\n",
        "            target_candidates_num = l.sum(dim=1).detach().cpu()\n",
        "        groups, candidates, group_candidates_scores = self.get_candidates(group_logits,\n",
        "                                                                          group_gd=group_labels if is_training else None)\n",
        "        if is_training:\n",
        "            bs = 0\n",
        "            new_labels = []\n",
        "            for i, n in enumerate(target_candidates_num.numpy()):\n",
        "                be = bs + n\n",
        "                c = set(target_candidates[bs: be].numpy())\n",
        "                c2 = candidates[i]\n",
        "                new_labels.append(torch.tensor([1.0 if i in c else 0.0 for i in c2 ]))\n",
        "                if len(c) != new_labels[-1].sum():\n",
        "                    s_c2 = set(c2)\n",
        "                    for cc in list(c):\n",
        "                        if cc in s_c2:\n",
        "                            continue\n",
        "                        for j in range(new_labels[-1].shape[0]):\n",
        "                            if new_labels[-1][j].item() != 1:\n",
        "                                c2[j] = cc\n",
        "                                new_labels[-1][j] = 1.0\n",
        "                                break\n",
        "                bs = be\n",
        "            labels = torch.stack(new_labels).cuda()\n",
        "        candidates, group_candidates_scores =  torch.LongTensor(candidates).cuda(), torch.Tensor(group_candidates_scores).cuda()\n",
        "\n",
        "        emb = self.l1(out)\n",
        "        embed_weights = self.embed(candidates) # N, sampled_size, H\n",
        "        emb = emb.unsqueeze(-1)\n",
        "        logits = torch.bmm(embed_weights, emb).squeeze(-1)\n",
        "\n",
        "        if is_training:\n",
        "            loss_fn = torch.nn.BCEWithLogitsLoss()\n",
        "            loss = loss_fn(logits, labels) + loss_fn(group_logits, group_labels)\n",
        "            return logits, loss\n",
        "        else:\n",
        "            candidates_scores = torch.sigmoid(logits)\n",
        "            candidates_scores = candidates_scores * group_candidates_scores\n",
        "            return group_logits, candidates, candidates_scores\n",
        "\n",
        "    def swa_init(self):\n",
        "        self.swa_state = {'models_num': 1}\n",
        "        for n, p in self.named_parameters():\n",
        "            self.swa_state[n] = p.data.cpu().clone().detach()\n",
        "\n",
        "    def swa_step(self):\n",
        "        if 'models_num' not in self.swa_state:\n",
        "            return\n",
        "        self.swa_state['models_num'] += 1\n",
        "        beta = 1.0 / self.swa_state['models_num']\n",
        "        with torch.no_grad():\n",
        "            for n, p in self.named_parameters():\n",
        "                self.swa_state[n].mul_(1.0 - beta).add_(beta, p.data.cpu())\n",
        "\n",
        "    def swa_swap_params(self):\n",
        "        if 'models_num' not in self.swa_state:\n",
        "            return\n",
        "        for n, p in self.named_parameters():\n",
        "            self.swa_state[n], p.data =  self.swa_state[n].cpu(), p.data.cpu()\n",
        "            self.swa_state[n], p.data =  p.data.cpu(), self.swa_state[n].cuda()\n",
        "\n",
        "    def get_tokenizer(self):\n",
        "        print('load bert-base-uncased tokenizer')\n",
        "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "        return tokenizer\n",
        "\n",
        "    def get_accuracy(self, candidates, logits, labels):\n",
        "        if candidates is not None:\n",
        "            candidates = candidates.detach().cpu()\n",
        "        scores, indices = torch.topk(logits.detach().cpu(), k=10)\n",
        "\n",
        "        acc1, acc3, acc5, total = 0, 0, 0, 0\n",
        "        for i, l in enumerate(labels):\n",
        "            l = set(np.nonzero(l)[0])\n",
        "\n",
        "            if candidates is not None:\n",
        "                labels = candidates[i][indices[i]].numpy()\n",
        "            else:\n",
        "                labels = indices[i, :5].numpy()\n",
        "\n",
        "            acc1 += len(set([labels[0]]) & l)\n",
        "            acc3 += len(set(labels[:3]) & l)\n",
        "            acc5 += len(set(labels[:5]) & l)\n",
        "            total += 1\n",
        "\n",
        "        return total, acc1, acc3, acc5\n",
        "\n",
        "    def one_epoch(self, epoch, dataloader, optimizer,\n",
        "                  mode='train', eval_loader=None, eval_step=20000):\n",
        "\n",
        "        bar = tqdm(total=len(dataloader))\n",
        "        p1, p3, p5 = 0, 0, 0\n",
        "        g_p1, g_p3, g_p5 = 0, 0, 0\n",
        "        total, acc1, acc3, acc5 = 0, 0, 0, 0\n",
        "        g_acc1, g_acc3, g_acc5 = 0, 0, 0\n",
        "        train_loss = 0\n",
        "\n",
        "        if mode == 'train':\n",
        "            self.train()\n",
        "        else:\n",
        "            self.eval()\n",
        "\n",
        "        if self.use_swa and epoch == self.swa_warmup_epoch and mode == 'train':\n",
        "            self.swa_init()\n",
        "\n",
        "        if self.use_swa and mode == 'eval':\n",
        "            self.swa_swap_params()\n",
        "\n",
        "        pred_scores, pred_labels = [], []\n",
        "        bar.set_description(f'{mode}-{epoch}')\n",
        "\n",
        "        with torch.set_grad_enabled(mode == 'train'):\n",
        "            # print(dataloader)\n",
        "            for step, data in enumerate(dataloader):\n",
        "                # print(step)\n",
        "                # print(data)\n",
        "                # print(\"FFFFF\")\n",
        "                batch = tuple(t for t in data)\n",
        "                # print(len(batch))\n",
        "                have_group = len(batch) > 4\n",
        "                inputs = {'input_ids':      batch[0].cuda(),\n",
        "                          'attention_mask': batch[1].cuda(),\n",
        "                          'token_type_ids': batch[2].cuda()}\n",
        "                if mode == 'train':\n",
        "                    inputs['labels'] = batch[3].cuda()\n",
        "                    if self.group_y is not None:\n",
        "                        # print(self.group_y)\n",
        "                        # print(\"HEREEEEEEEEEEEEEE\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\")\n",
        "                        inputs['group_labels'] = batch[4].cuda()\n",
        "                        inputs['candidates'] = batch[5].cuda()\n",
        "\n",
        "                outputs = self(**inputs)\n",
        "\n",
        "                bar.update(1)\n",
        "\n",
        "                if mode == 'train':\n",
        "                    loss = outputs[1]\n",
        "                    loss /= self.update_count\n",
        "                    train_loss += loss.item()\n",
        "\n",
        "                    with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
        "                        scaled_loss.backward()\n",
        "\n",
        "                    if step % self.update_count == 0:\n",
        "                        optimizer.step()\n",
        "                        self.zero_grad()\n",
        "\n",
        "                    if step % eval_step == 0 and eval_loader is not None and step != 0:\n",
        "                        results = self.one_epoch(epoch, eval_loader, optimizer, mode='eval')\n",
        "                        p1, p3, p5 = results[3:6]\n",
        "                        g_p1, g_p3, g_p5 = results[:3]\n",
        "                        # if self.group_y is not None:\n",
        "                        #     log.log(f'{epoch:>2} {step:>6}: {p1:.4f}, {p3:.4f}, {p5:.4f}'\n",
        "                        #             f' {g_p1:.4f}, {g_p3:.4f}, {g_p5:.4f}')\n",
        "                        # else:\n",
        "                        #     log.log(f'{epoch:>2} {step:>6}: {p1:.4f}, {p3:.4f}, {p5:.4f}')\n",
        "                        # NOTE: we don't reset model to train mode and keep model in eval mode\n",
        "                        # which means all dropout will be remove after `eval_step` in every epoch\n",
        "                        # this tricks makes LightXML converge fast\n",
        "                        # self.train()\n",
        "\n",
        "                    if self.use_swa and step % self.swa_update_step == 0:\n",
        "                        self.swa_step()\n",
        "\n",
        "                    bar.set_postfix(loss=loss.item())\n",
        "                elif self.group_y is None:\n",
        "                    logits = outputs\n",
        "                    if mode == 'eval':\n",
        "                        labels = batch[3]\n",
        "                        _total, _acc1, _acc3, _acc5 =  self.get_accuracy(None, logits, labels.cpu().numpy())\n",
        "                        total += _total; acc1 += _acc1; acc3 += _acc3; acc5 += _acc5\n",
        "                        p1 = acc1 / total\n",
        "                        p3 = acc3 / total / 3\n",
        "                        p5 = acc5 / total / 5\n",
        "                        bar.set_postfix(p1=p1, p3=p3, p5=p5)\n",
        "                    elif mode == 'test':\n",
        "                        pred_scores.append(logits.detach().cpu())\n",
        "                else:\n",
        "                    group_logits, candidates, logits = outputs\n",
        "\n",
        "                    if mode == 'eval':\n",
        "                        # print(batch)\n",
        "                        labels = batch[3]\n",
        "                        group_labels = batch[4]\n",
        "\n",
        "                        _total, _acc1, _acc3, _acc5 = self.get_accuracy(candidates, logits, labels.cpu().numpy())\n",
        "                        total += _total; acc1 += _acc1; acc3 += _acc3; acc5 += _acc5\n",
        "                        p1 = acc1 / total\n",
        "                        p3 = acc3 / total / 3\n",
        "                        p5 = acc5 / total / 5\n",
        "\n",
        "                        _, _g_acc1, _g_acc3, _g_acc5 = self.get_accuracy(None, group_logits, group_labels.cpu().numpy())\n",
        "                        g_acc1 += _g_acc1; g_acc3 += _g_acc3; g_acc5 += _g_acc5\n",
        "                        g_p1 = g_acc1 / total\n",
        "                        g_p3 = g_acc3 / total / 3\n",
        "                        g_p5 = g_acc5 / total / 5\n",
        "                        bar.set_postfix(p1=p1, p3=p3, p5=p5, g_p1=g_p1, g_p3=g_p3, g_p5=g_p5)\n",
        "                    elif mode == 'test':\n",
        "                        _scores, _indices = torch.topk(logits.detach().cpu(), k=100)\n",
        "                        _labels = torch.stack([candidates[i][_indices[i]] for i in range(_indices.shape[0])], dim=0)\n",
        "                        pred_scores.append(_scores.cpu())\n",
        "                        pred_labels.append(_labels.cpu())\n",
        "\n",
        "\n",
        "        if self.use_swa and mode == 'eval':\n",
        "            self.swa_swap_params()\n",
        "        bar.close()\n",
        "\n",
        "        if mode == 'eval':\n",
        "            return g_p1, g_p3, g_p5, p1, p3, p5\n",
        "        elif mode == 'test':\n",
        "            return torch.cat(pred_scores, dim=0).numpy(), torch.cat(pred_labels, dim=0).numpy() if len(pred_labels) != 0 else None\n",
        "        elif mode == 'train':\n",
        "            return train_loss\n",
        "\n",
        "def train(model, df, label_map,lr,epoch,group_y,max_len,group_y_candidate_num,batch,epochs,valid,eval_step,tokenizer):\n",
        "    tokenizer = model.get_tokenizer()\n",
        "\n",
        "    # group_y = load_group(args.dataset, args.group_y_group)\n",
        "    train_d = MDataset(df, 'train', tokenizer, label_map, max_len, group_y=group_y,\n",
        "                           candidates_num=group_y_candidate_num)#, token_type_ids=token_type_ids)\n",
        "    test_d = MDataset(df, 'test', tokenizer, label_map, max_len, group_y=group_y,\n",
        "                           candidates_num=group_y_candidate_num)#, token_type_ids=token_type_ids)\n",
        "\n",
        "    train_d.tokenizer = BertWordPieceTokenizer(\n",
        "                \"/content/drive/MyDrive/EUR-Lex/bert-base-uncased-vocab.txt\",\n",
        "                lowercase=True)\n",
        "    test_d.tokenizer = BertWordPieceTokenizer(\n",
        "                \"/content/drive/MyDrive/EUR-Lex/bert-base-uncased-vocab.txt\",\n",
        "                lowercase=True)\n",
        "    # print(train_d)\n",
        "    # print(\"end\")\n",
        "    trainloader = DataLoader(train_d, batch_size=batch, num_workers=2,\n",
        "                              shuffle=True)\n",
        "    testloader = DataLoader(test_d, batch_size=batch, num_workers=2,\n",
        "                            shuffle=False)\n",
        "    valid_d = MDataset(df, 'valid', tokenizer, label_map, max_len)\n",
        "    validloader = DataLoader(valid_d, batch_size=batch, num_workers=2,\n",
        "                              shuffle=False)\n",
        "\n",
        "    model.cuda()\n",
        "    no_decay = ['bias', 'LayerNorm.weight']\n",
        "    optimizer_grouped_parameters = [\n",
        "        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "        ]\n",
        "    optimizer = AdamW(optimizer_grouped_parameters, lr=lr)#, eps=1e-8)\n",
        "\n",
        "    model, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\")\n",
        "    max_only_p5 = 0\n",
        "    for epoch in range(0, epoch+5):\n",
        "        train_loss = model.one_epoch(epoch, trainloader, optimizer, mode='train',\n",
        "                                     eval_loader=testloader,\n",
        "                                     eval_step=eval_step)\n",
        "        ev_result = model.one_epoch(epoch, testloader, optimizer, mode='eval')\n",
        "\n",
        "        g_p1, g_p3, g_p5, p1, p3, p5 = ev_result\n",
        "\n",
        "        # log_str = f'{epoch:>2}: {p1:.4f}, {p3:.4f}, {p5:.4f}, train_loss:{train_loss}'\n",
        "        # if args.dataset in ['wiki500k', 'amazon670k']:\n",
        "        #     log_str += f' {g_p1:.4f}, {g_p3:.4f}, {g_p5:.4f}'\n",
        "        # if args.valid:\n",
        "        #     log_str += ' valid'\n",
        "        # LOG.log(log_str)\n",
        "\n",
        "        if max_only_p5 < p5:\n",
        "            max_only_p5 = p5\n",
        "            # model.save_model(f'models/model-{get_exp_name()}.bin')\n",
        "\n",
        "        if epoch >= epochs + 5 and max_only_p5 != p5:\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GoK5L8zaxrd7",
        "outputId": "8d258280-59cc-4c59-ec95-246722b60d91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "valid size 4000\n",
            "load EURLEX dataset with 11449 train 3865 test with 3956 labels done\n"
          ]
        }
      ],
      "source": [
        "train_df, valid_df = train_test_split(df[df['dataType'] == 'train'],\n",
        "                                              test_size=4000,\n",
        "                                              random_state=1240)\n",
        "df.iloc[valid_df.index.values, 2] = 'valid'\n",
        "print('valid size', len(df[df['dataType'] == 'valid']))\n",
        "print(f'load EURLEX dataset with '\n",
        "          f'{len(df[df.dataType ==\"train\"])} train {len(df[df.dataType ==\"test\"])} test with {len(label_map)} labels done')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "UjYhlVe3RD4h"
      },
      "outputs": [],
      "source": [
        "_group_y = []\n",
        "for idx, labels in enumerate(new_group):\n",
        "  _group_y.append([])\n",
        "  for label in labels:\n",
        "    _group_y[-1].append(label_map[label])\n",
        "    # _group_y[-1] = np.array(_group_y[-1])\n",
        "for i in range(len(_group_y)):\n",
        "  _group_y[i] = np.array(_group_y[i])\n",
        "group_y_init = np.array(_group_y,dtype=object)\n",
        "# group_y_init"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eSLmUaarvH3L",
        "outputId": "2141f221-217e-4ade-d889-97fe038c7d7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "swa True 4 3000 {}\n",
            "update_count 1\n",
            "hidden dim: 500\n",
            "label goup numbers: 256\n",
            "load bert-base-uncased tokenizer\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "/content/apex/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)\n",
            "  warnings.warn(msg, DeprecatedFeatureWarning)\n",
            "/content/apex/apex/amp/scaler.py:56: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n",
            "  self._overflow_buf = torch.cuda.IntTensor([0])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O1\n",
            "cast_model_type        : None\n",
            "patch_torch_functions  : True\n",
            "keep_batchnorm_fp32    : None\n",
            "master_weights         : None\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O1\n",
            "cast_model_type        : None\n",
            "patch_torch_functions  : True\n",
            "keep_batchnorm_fp32    : None\n",
            "master_weights         : None\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train-0: 100%|██████████| 716/716 [07:25<00:00,  1.61it/s, loss=0.105]\n",
            "eval-0: 100%|██████████| 242/242 [00:51<00:00,  4.74it/s, g_p1=0.624, g_p3=0.471, g_p5=0.374, p1=0.372, p3=0.291, p5=0.243]\n",
            "train-1: 100%|██████████| 716/716 [07:22<00:00,  1.62it/s, loss=0.0947]\n",
            "eval-1: 100%|██████████| 242/242 [00:51<00:00,  4.74it/s, g_p1=0.803, g_p3=0.611, g_p5=0.476, p1=0.58, p3=0.447, p5=0.355]\n",
            "train-2: 100%|██████████| 716/716 [07:21<00:00,  1.62it/s, loss=0.112]\n",
            "eval-2: 100%|██████████| 242/242 [00:51<00:00,  4.74it/s, g_p1=0.844, g_p3=0.65, g_p5=0.507, p1=0.647, p3=0.5, p5=0.406]\n",
            "train-3: 100%|██████████| 716/716 [07:22<00:00,  1.62it/s, loss=0.0733]\n",
            "eval-3: 100%|██████████| 242/242 [00:50<00:00,  4.76it/s, g_p1=0.871, g_p3=0.677, g_p5=0.527, p1=0.714, p3=0.559, p5=0.45]\n",
            "train-4:   0%|          | 1/716 [00:00<11:20,  1.05it/s]/content/apex/apex/amp/wrap.py:111: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1519.)\n",
            "  return orig_fn(arg0, *new_args, **kwargs)\n",
            "train-4: 100%|██████████| 716/716 [07:23<00:00,  1.62it/s, loss=0.0799]\n",
            "eval-4: 100%|██████████| 242/242 [00:51<00:00,  4.67it/s, g_p1=0.869, g_p3=0.676, g_p5=0.527, p1=0.713, p3=0.558, p5=0.448]\n",
            "train-5: 100%|██████████| 716/716 [07:22<00:00,  1.62it/s, loss=0.0665]\n",
            "eval-5: 100%|██████████| 242/242 [00:51<00:00,  4.66it/s, g_p1=0.887, g_p3=0.693, g_p5=0.542, p1=0.743, p3=0.585, p5=0.473]\n",
            "train-6:  57%|█████▋    | 409/716 [04:13<03:08,  1.63it/s, loss=0.0488]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train-6: 100%|██████████| 716/716 [07:22<00:00,  1.62it/s, loss=0.0652]\n",
            "eval-6: 100%|██████████| 242/242 [00:51<00:00,  4.67it/s, g_p1=0.892, g_p3=0.704, g_p5=0.548, p1=0.764, p3=0.603, p5=0.489]\n",
            "train-7: 100%|██████████| 716/716 [07:22<00:00,  1.62it/s, loss=0.0477]\n",
            "eval-7: 100%|██████████| 242/242 [00:51<00:00,  4.66it/s, g_p1=0.893, g_p3=0.71, g_p5=0.554, p1=0.777, p3=0.615, p5=0.502]\n",
            "train-8: 100%|██████████| 716/716 [07:22<00:00,  1.62it/s, loss=0.035]\n",
            "eval-8: 100%|██████████| 242/242 [00:51<00:00,  4.69it/s, g_p1=0.898, g_p3=0.715, g_p5=0.557, p1=0.787, p3=0.631, p5=0.515]\n",
            "train-9: 100%|██████████| 716/716 [07:22<00:00,  1.62it/s, loss=0.0267]\n",
            "eval-9: 100%|██████████| 242/242 [00:51<00:00,  4.69it/s, g_p1=0.898, g_p3=0.719, g_p5=0.56, p1=0.795, p3=0.642, p5=0.524]\n",
            "train-10: 100%|██████████| 716/716 [07:22<00:00,  1.62it/s, loss=0.0297]\n",
            "eval-10: 100%|██████████| 242/242 [00:51<00:00,  4.66it/s, g_p1=0.899, g_p3=0.721, g_p5=0.562, p1=0.801, p3=0.65, p5=0.53]\n",
            "train-11: 100%|██████████| 716/716 [07:22<00:00,  1.62it/s, loss=0.0224]\n",
            "eval-11: 100%|██████████| 242/242 [00:51<00:00,  4.68it/s, g_p1=0.9, g_p3=0.723, g_p5=0.564, p1=0.81, p3=0.659, p5=0.535]\n",
            "train-12: 100%|██████████| 716/716 [07:23<00:00,  1.62it/s, loss=0.0165]\n",
            "eval-12: 100%|██████████| 242/242 [00:51<00:00,  4.66it/s, g_p1=0.901, g_p3=0.726, g_p5=0.565, p1=0.816, p3=0.664, p5=0.541]\n",
            "train-13: 100%|██████████| 716/716 [07:23<00:00,  1.61it/s, loss=0.0111]\n",
            "eval-13: 100%|██████████| 242/242 [00:51<00:00,  4.66it/s, g_p1=0.902, g_p3=0.728, g_p5=0.566, p1=0.816, p3=0.67, p5=0.545]\n",
            "train-14: 100%|██████████| 716/716 [07:23<00:00,  1.61it/s, loss=0.0174]\n",
            "eval-14: 100%|██████████| 242/242 [00:51<00:00,  4.67it/s, g_p1=0.901, g_p3=0.731, g_p5=0.567, p1=0.819, p3=0.675, p5=0.549]\n",
            "train-15: 100%|██████████| 716/716 [07:23<00:00,  1.61it/s, loss=0.0177]\n",
            "eval-15: 100%|██████████| 242/242 [00:52<00:00,  4.64it/s, g_p1=0.901, g_p3=0.731, g_p5=0.568, p1=0.823, p3=0.679, p5=0.553]\n",
            "train-16: 100%|██████████| 716/716 [07:24<00:00,  1.61it/s, loss=0.0122]\n",
            "eval-16: 100%|██████████| 242/242 [00:51<00:00,  4.67it/s, g_p1=0.903, g_p3=0.732, g_p5=0.569, p1=0.821, p3=0.681, p5=0.558]\n",
            "train-17:  77%|███████▋  | 548/716 [05:39<01:43,  1.62it/s, loss=0.0154]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1048576.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train-17: 100%|██████████| 716/716 [07:22<00:00,  1.62it/s, loss=0.0152]\n",
            "eval-17: 100%|██████████| 242/242 [00:51<00:00,  4.68it/s, g_p1=0.902, g_p3=0.734, g_p5=0.57, p1=0.826, p3=0.684, p5=0.56]\n",
            "train-18: 100%|██████████| 716/716 [07:23<00:00,  1.62it/s, loss=0.00952]\n",
            "eval-18: 100%|██████████| 242/242 [00:51<00:00,  4.67it/s, g_p1=0.903, g_p3=0.734, g_p5=0.571, p1=0.827, p3=0.687, p5=0.563]\n",
            "train-19: 100%|██████████| 716/716 [07:23<00:00,  1.61it/s, loss=0.0102]\n",
            "eval-19: 100%|██████████| 242/242 [00:51<00:00,  4.67it/s, g_p1=0.903, g_p3=0.734, g_p5=0.571, p1=0.826, p3=0.69, p5=0.566]\n",
            "train-20:  94%|█████████▍| 676/716 [06:58<00:24,  1.63it/s, loss=0.0178] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1048576.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train-20: 100%|██████████| 716/716 [07:23<00:00,  1.62it/s, loss=0.0102]\n",
            "eval-20: 100%|██████████| 242/242 [00:51<00:00,  4.69it/s, g_p1=0.902, g_p3=0.735, g_p5=0.572, p1=0.826, p3=0.691, p5=0.568]\n",
            "train-21: 100%|██████████| 716/716 [07:23<00:00,  1.62it/s, loss=0.0241]\n",
            "eval-21: 100%|██████████| 242/242 [00:51<00:00,  4.66it/s, g_p1=0.903, g_p3=0.735, g_p5=0.572, p1=0.827, p3=0.694, p5=0.569]\n",
            "train-22:  64%|██████▍   | 458/716 [04:43<02:38,  1.63it/s, loss=0.00887]"
          ]
        }
      ],
      "source": [
        "# print(group_y)\n",
        "model = LightXML(model_bert,n_labels=len(label_map), group_y=group_y_init,\n",
        "                          update_count=1,\n",
        "                          use_swa=True, swa_warmup_epoch=4, swa_update_step=3000,\n",
        "                          candidates_topk=32,\n",
        "                          hidden_dim=500)\n",
        "# model = LightXML(model_bert,n_labels=len(label_map),\n",
        "#                          update_count=1,\n",
        "#                          use_swa=True, swa_warmup_epoch=10, swa_update_step=200)\n",
        "train(model, df, label_map,eps,EPOCH,new_group,max_len,group_y_candidate_num,BATCH,EPOCH,False,eval_step,tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train = [i.replace('\\n', '').split() for i in open('/content/drive/MyDrive/EUR-Lex/train_labels.txt')]\n",
        "test = [i.replace('\\n', '').split() for i in open('/content/drive/MyDrive/EUR-Lex/test_labels.txt')]\n",
        "\n",
        "def get_unique_strings(list_of_lists):\n",
        "    # Flatten the list of lists into a single list\n",
        "    flat_list = [string for sublist in list_of_lists for string in sublist]\n",
        "\n",
        "    # Use set to get unique strings\n",
        "    unique_strings = list(set(flat_list))\n",
        "\n",
        "    return unique_strings\n",
        "\n",
        "train = get_unique_strings(train)\n",
        "test = get_unique_strings(test)\n"
      ],
      "metadata": {
        "id": "Ab1ci4a0X5Gn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_unique_strings(list1, list2):\n",
        "    # Convert both lists to sets for efficient membership checking\n",
        "    set1 = set(list1)\n",
        "    set2 = set(list2)\n",
        "\n",
        "    # Find the difference (strings in set2 but not in set1)\n",
        "    unique_strings = list(set2 - set1)\n",
        "\n",
        "    return unique_strings\n",
        "len(find_unique_strings(train,test))"
      ],
      "metadata": {
        "id": "6v1rxfkPYvc-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rCT1Vr4BZOXP"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1f0fb671d7a8498aba4d12f7ad65bb32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a8749cfc192040c08b25d33704ac17a0",
              "IPY_MODEL_31eac8c3bb714442bbfe672b9ea5bceb",
              "IPY_MODEL_a5bbf624f6c94886a0dbedb5558950b2"
            ],
            "layout": "IPY_MODEL_b87ed1ce0e914768b107cc088415219f"
          }
        },
        "d4e380b1a16941ffb9feda018542e17c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ea2be0eb6f7a458ab5d7d0b3b6113768",
              "IPY_MODEL_7ff711c84a5345a7801613388f45b7bd",
              "IPY_MODEL_c52cbf5028ac4fc0882263b5a7fef1df"
            ],
            "layout": "IPY_MODEL_c173a572291c4213beddea6c9e1327ff"
          }
        },
        "f8517e8bc2ae44c4a97d487477f988d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_63539ae3c13040edacc57d3d10c3b814",
              "IPY_MODEL_9a902cb7c9d44e7391ec963efc10b17f",
              "IPY_MODEL_acd4304316e04bfe9c31c93879d75bc6"
            ],
            "layout": "IPY_MODEL_8f1ec19843684023bc3a040994b119f7"
          }
        },
        "dee7bfaf69474153b6d6d7a9a2887df4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_41bd514124c049098f11c0d0ef548f4e",
              "IPY_MODEL_5d8cdd692b9f468a8f40487d2d5318c1",
              "IPY_MODEL_7e81080c19cb4184afc0490991e2af75"
            ],
            "layout": "IPY_MODEL_e44b80dedf8643e6a779758697925d17"
          }
        },
        "3fd5b2c8702c45e2bd9ab8b45c5aa784": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_caf082bda6e045fd809b9c961852893a",
              "IPY_MODEL_c0a94ab8623847a19f91f4b56ae33256",
              "IPY_MODEL_38eeba6e19324e859a46951c32b3c70a"
            ],
            "layout": "IPY_MODEL_c383eaff7c2f402c8eb13ac4a4111a0b"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}